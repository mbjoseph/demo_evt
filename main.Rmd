---
title: "Understanding and predicting extreme wildfires in the contiguous United States"
bibliography: library.bib
output:
    bookdown::pdf_document2:
      keep_tex: true
      toc: no
      includes:
          in_header: header.sty
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(knitr)
library(kableExtra)
library(assertthat)
```

```{r load-count-test-intervals, message=FALSE, warning=FALSE}
count_test_intervals <- read_csv('data/processed/count_test_intervals.csv')
```

\begin{abstract}
Wildfires are becoming more frequent and getting larger in many parts of the globe, but predicting where and when extreme events might occur remains difficult. 
To explain and predict wildfire extremes across the contiguous United States, we integrate a 30 year wildfire occurrence record with meteorological and housing data in a spatiotemporal Bayesian model with spatially varying nonlinear effects. 
We find that dryness and air temperature strongly regulate wildfire risk, with precipitation, and human housing density playing weaker roles.
Statistically, most of the variability in the chance of an extreme wildfire results from changes in fire frequency, rather than changes in the underlying distribution of expected fire sizes. 
Meteorologically, extreme events occur when conditions are exceptionally hot and dry. 
This model attains `r 100 * round(mean(count_test_intervals$in_interval), 3)`\% interval coverage for the number of fires over 1000 acres in a withheld data set over a five-year prediction time horizon. 
We argue that recent wildfire extremes need not be surprising, and future extremes might be predictable if accurate meteorological forecasts are available to drive models of fire occurrence and size.
\end{abstract}

\linenumbers


# Introduction {-}

Wildfire frequency and burned area has increased over the past couple decades in the United States [@dennison2014large; @westerling2016increasing], and other areas globally [@krawchuk2009global; @pechony2010driving]. 
In addition to the ecological and smoke impacts associated with increased burned area, there has been an increasing interest in extreme wildfires [@williams2013exploring] - i.e.., fires with the largest burn area over a spatiotemporal domain - given the hazards they pose in some ecosystems as well as to human lives and infrastructure [@kochi2010economic; @diaz2012economic].
While case studies of particular extremes can provide insight into what caused observed events in the context of local drivers [@peterson20152013], predicting future extremes across heterogeneous regions is remains difficult but important from the perspective of managing disaster related resource allocation at a national level. 

Factors driving wildfire extremes vary in space and time [@barbero2014modeling], but it is unclear how best to account for this in a predictive model. 
Previous efforts have used region-specific models [@bermudez2009spatial], temporal or spatial models [@mendes2010spatial], and spatial models with year as a covariate [@diaz2016modeling]. 
Recently, spatiotemporal models have been described with linear, spatially constant covariate effects [@serra2014spatio; @serra2014spatioB]. 
However, linear, spatially constant effects are unappealing in large spatial domains and in the context of nonlinear drivers of wildfires [@fosberg1978weather, @goodrick2002modification, @preisler2004probability; @preisler2007statistical; @balshi2009assessing; @krawchuk2009global; @vilar2010model; @woolford2011spatio; @woolford2014lightning]. 
For example, global wildfire probability shows a hump-shaped relationship with temperature and moisture [@moritz2012climate]. 
Interactions among drivers also impose nonlinearity, e.g., in hot and dry climates, sparse fuels inhibit fire [@mclaughlin1982effects], but in cold and wet climates, fires are energy limited [@Krawchuk2011]. 

Prediction is also complicated by uncertainty in which distribution(s) to use to assign probabilities to extreme events. 
The generalized Pareto distribution (GPD) has frequently been used [@bermudez2009spatial; @jiang2011extreme], but the GPD requires a threshold to delineate extreme vs. not extreme events [@davison1990models, @coles2014introduction]. 
The utility and validity of one threshold for extremes in a heterogeneous region is debatable [@tedim2018defining]. 
Recently proposed metastatistical extreme value (MEV) approaches do not require such a threshold [@marani2015metastatistical; @zorzetto2016emergence]. 
In the MEV framework, the occurrence and size of future events, and the parameters of their distributions are treated as random variables which imply a distribution for extremes. 
This approach has roots in compound distributions [@dubey1970compound; @wiitala1999assessing], doubly stochastic processes [@cox1980point], superstatistics [@beck2003superstatistics], and the Bayesian posterior predictive distribution [@gelman2013bayesian]. 

Here, we extend the MEV perspective to account for non-linear, spatially varying wildfire dynamics with the goal of predicting and explaining extreme wildfire events across the contiguous United States.
We aim to predict occurrence (where and when), and magnitude (burn area) of large wildfires over a monthly time scale. 
Such information could help prioritize reactive fire suppression resources or inform proactive wildfire risk mitigation.

# Methods {-}

## Data description {-}

```{r read-count-data, message=FALSE, warning=FALSE}
train_counts <- read_rds('data/processed/train_counts.rds')
holdout_counts <- read_rds('data/processed/holdout_counts.rds')
count_df <- full_join(train_counts, holdout_counts)
n_fires_total <- sum(count_df$n_fire)
```

We acquired wildfire event data for the contiguous United States from the Monitoring Trends in Burn Severity (MTBS, www.mtbs.gov) program [@eidenshink20071145801], which includes spatiotemporal information on the occurrence of wildfires in the United States from 1984 to 2015. 
The MTBS data contain fires greater than 1000 acres in the western US and greater than 500 acres in the eastern US. 
For consistency across the US, we discarded all records in the MTBS data less than 1000 acres, resulting in `r format(n_fires_total, big.mark = ',', scientific = FALSE)` fire events under consideration (Figure \ref{study-region}A).

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/maps.png}
\caption{A. Fire ignition locations are shown as points across the study region. Colors in panels B, C, and D show different level 1, 2, and 3 ecoregions respectively.}
\label{study-region}
\end{figure}

To explain fire size and occurrence, we used a combination of meteorological variables including humidity, air temperature, precipitation, and wind speed. 
These variables were selected on the basis of previous work, and also with an aim to capture directly the effects of easily interpretable meteorological quantities (as compared to an index of fire weather, for example). 
Meteorological layers were acquired from the gridMET data [@abatzoglou2013development] that blends monthly high-spatial resolution (~4-km) climate data from the Parameter-elevation Relationships on Independent Slopes Model [@daly2008physiographically] with high-temporal resolution (hourly) data from the National Land Data Assimilation System (NLDAS2) using climatologically aided interpolation. 
The resultant products are a suite of surface meteorological variables summarized at the daily time step and at a 4-km spatial grain. 
Daily values of total precipitation, minimum relative humidity, mean wind speed, and maximum air temperature were averaged at a monthly time step for each of 84 Environmental Protection Agency level 3 ecoregions for each month from 1984 to 2015 [@omernik1987ecoregions; @omernik2014ecoregions].
For each month, we computed the mean amount of precipitation over the past 12 months to get an indicator of long term water availability, which might relate to fuel biomass availability for some ecoregions. 

We also used publicly available housing density estimates that were generated based on the US 2000 decennial census as explanatory variables [@radeloff2010housing]. 
These are provided at decadal time steps, and spatially at the level of census partial block groups. 
To generate approximate measures of housing density at monthly time intervals, we used a simple linear interpolation over time, and aggregated spatially across block groups to compute mean housing density for each ecoregion in each month. 

## Model development {-}

We built two types of models: one describing the occurrence of fires within an ecoregion over time (i.e., the total number of fires occurring in each ecoregion for each month from 1984 - 2015), and another describing the size of each wildfire in each ecoregion and month.
For the occurrence models, the response variable was a count (number of fires), and for the burn area models, the response was a continuous positive quantity (burn area of each fire event). 
Modeling the size of each fire, rather than the mean or total burn area of all fires within a spatiotemporal unit allows us to explore spatiotemporal variation in the distribution of fire sizes. 
We used the period from 1984 to 2009 for training, and the period from 2010 to 2015 was withheld to evaluate predictive performance. 

### Fire occurrence {-}

We constructed four models for fire occurrence and compared their predictive performance based on test-set log likelihood, and posterior predictive checks for the proportion of zeros, maximum count, and total count. 
The models differed in the distributions used in the likelihood, with each model representing counts in each month as a Poisson, negative binomial, zero-inflated Poisson, or zero-inflated negative binomial random variable. 

For spatial units (ecoregions) $s=1, ..., S$ and time steps (months) $t = 1, ..., T$, each model defines a probability mass function for $n_{s, t}$: the number of fires over 1000 acres in ecoregion $s$ and time step $t$. 
For each of the four count distributions under consideration, location parameters $\mu_{s, t}$ and (for zero-inflated models) structural zero inflation parameters $\pi_{s, t}$ were allowed to vary in space and time.
We used a log link function to ensure that $\mu_{s, t} > 0$, and a logit link function to ensure that $\pi_{s, t} \in (0, 1)$.
Concatenating over spatial and temporal units, so that $\bm{\mu} = (\mu_{s=1, t=1}, \mu_{s=2, t=1}, ..., \mu_{s=S, t=1}, \mu_{s=S, t=2}, ..., \mu_{s=S, t=T})$, and similarly for $\bm{\pi}$, we modeled distributional location and (when applicable) zero inflation parameters as: 

$$\log(\bm{\mu}) = \alpha^{(\mu)} + \matr{X} \bm{\beta}^{(\mu)} + \bm \phi^{(\mu)} + \log (\bm a),$$

$$\textnormal{logit}(\bm{\pi}) = \alpha^{(\pi)} + \matr{X} \bm{\beta}^{(\pi)}  + \bm \phi^{(\pi)},$$

where $\alpha^{(\mu)}$ and $\alpha^{(\pi)}$ are scalar intercept parameters, $\matr{X}$ is a known $(S \times T) \times p$ design matrix, where $p$ is the number of input features, $\bm{\beta}^{(\mu)}$ and $\bm{\beta}^{(\pi)}$ are column vector parameters of length $p$, $\bm \phi^{(\mu)}$  and $\bm \phi^{(\pi)}$ are column vector parameters of length $S \times T$ containing spatiotemporal adjustments, and $\bm a$ is a known offset vector of areas for spatial unit $s = 1, 2, ..., S$, repeated $T$ times.

### Burn area {-}

We developed multiple candidate models for burn area, each of which specified a different distribution for burn areas [@reed2002power; @hernandez2015statistical], including the generalized Pareto [@hosking1987parameter], tapered Pareto [@schoenberg2003distribution], lognormal, gamma, and Weibull distributions. 
We evaluated each model in terms of test set log likelihood, and posterior predictive checks for burn area extremes. 
We defined the response $y_{i}$ as the number of acres burned over 1000 for the $i^{th}$ fire event, which occurred in spatial unit $s_i$ and time step $t_i$.

Because each burn area distribution has a different parameterization, we included covariate effects in a distribution-specific way. 
For the generalized Pareto distribution (GPD), we assumed a positive shape parameter, leading to a Lomax distribution for exceedances [@bermudez2009spatial]. 
The GPD and Lomax shape parameters are related by $\kappa^{(GPD)}=1 / \kappa^{(L)}$, and the GPD scale parameter is related to the Lomax scale and shape parameters ($\sigma^{(GPD)} = \sigma^{(L)} / \kappa^{(L)}$). 
We introduced covariate dependence via the Lomax scale parameter using a log link. 
For event $i$, $\log(\sigma^{(L)}_{i}) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$, where $\alpha$ is an intercept parameter, $\bm{\beta}$ is a length $p$ vector of coefficients,  $\bm{X}_{(s_i, t_i)}$ is a row vector from $\matr{X}$, and $\phi_{s_i, t_i}$ is a spatiotemporal adjustment for $s_i$ and $t_i$. 
For the tapered Pareto model, we modeled the shape parameter as $\log(\kappa_i) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$. 
The lognormal model included covariate dependence via the location parameter: $\mu_i = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$. 
The gamma model used a log link for the expected value: $\log (E(y_i)) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$. 
Last, we modeled the Weibull scale parameter as $\log(\sigma_i) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$.
More detail on the parameterization of each burn area distribution is provided in the Supporting Information. 


### Accounting for nonlinear forcing {-}

The design matrix $\matr{X}$ was constructed to allow for spatially varying nonlinear effects of housing density and meteorological drivers. 
We used B-splines to account for nonlinearity and allowed the coefficients for each basis vector to vary spatially [@wood2017]. 
Then, we constructed univariate B-splines for log housing density, wind speed, same month precipitation, previous 12 month precipitation, air temperature, and humidity, with five degrees of freedom (including an intercept) for each variable.
This step generated 30 basis vectors (five for each of six variables). 

```{r}
colnamesX <- read_rds('data/processed/colnamesX.rds')
X <- read_rds('data/processed/X.rds')
X_pct_zero <- mean(X == 0)
```

To allow for spatial variation in these nonlinear effects, we added interaction effects between each of the basis vectors and ecoregions [@brezger2006generalized; @kneib2009variable].
The hierarchical nesting of ecoregion designations (Figure \ref{study-region}B-D) lends itself to such interactions. 
Conceptually, what happens in a level 3 ecoregion may be related to what happens in the level 2 ecoregion containing the level 3 region, the level 1 ecoregion containing the level 2 region, and a global effect. 
The coefficient associated with a basis vector for any level 3 ecoregion is the sum of a global effect, a level 1 ecoregion adjustment, a level 2 ecoregion adjustment, and a level 3 ecogregion adjustment. 
Thus, for every univariate basis vector, we included an interaction effect with ecoregion at each of the three ecoregion levels.
This is appealing because the ecoregion designation allows borrowing of information across space (level 3 ecoregions in a level 2 ecoregion are often adjacent), and for regions that are ecologically similar. 
We also included adjustments on the global intercept for each level 1, 2, and 3 ecoregion to account for spatial variation that is unrelated to climate or housing density. 
This specification induces sparsity in $\matr{X}$ that we exploit to increase the efficiency of computing $\bm \mu$ and $\bm \pi$. 
In total, $\matr{X}$ has $p=$ `r format(length(colnamesX), big.mark = ',', scientific = FALSE)` columns, with `r round(100 * X_pct_zero, 0)`\% zero entries.

## Prior specification {-}

To avoid overfitting, we employed a regularized horseshoe prior on the coefficients associated with the spatially varying nonlinear effects described above  [@piironen2017sparsity]. 
For the zero inflated count models, this was a multivariate horseshoe to allow information sharing between the zero inflated and distribution specific location parameters [@peltola2014hierarchical]. 
For the remaining count models and all burn area models, this was a univariate horseshoe prior. 
Spatiotemporal random effects were constructed using a temporally autoregressive, spatially intrinsically autoregressive formulation [@besag1995conditional; @banerjee2014hierarchical]. 
Details of these priors and the resulting joint distributions are provided in the Supporting Information.

## Posterior predictive inference and extremes {-}

We used the posterior predictive distribution to check each model and make inference on extremes. 
The posterior predictive distribution provides a distribution for replications of observed data ($y^{\textnormal{rep}}$), and predictions of future data [@gelman2013bayesian]. 
Conceptually, for a "good" model, $y^{\textnormal{rep}}$ should be similar to observed training data $y$, and future predictions should be similar to future data. 
Distributions over both quantities can be obtained by conditioning $y$ and marginalizing over model parameters $\theta$, e.g., $[y^{\textnormal{rep}} | y] = \int [y^{\textnormal{rep}} | \theta] [\theta | y] d\theta$.

Posterior predictive distributions facilitate model checks that compare predicted and observed test statistics [@gelman1996posterior].
To evaluate whether a model captures tail behavior, one can compare an empirical maximum ($T(y) = \textnormal{max}(y)$) to the predicted distribution of maxima $T(y^{\textnormal{rep}})$. 
We also include predictive checks for the proportion of zero counts, and totals for count and burn area models.
Posterior predictive inference for maxima is similar in spirit to the MEV approach. 
Both obtain a distribution over maxima by marginalizing over unknown parameters, including the number of events, size of each event, and parameters of their generative [@marani2015metastatistical].
However, a Bayesian approach explicitly conditions on the observed data to obtain a posterior distribution of parameters.
Seeing this connection is useful in the context of including priors and propagating uncertainty in derived parameters, including total burn areas and probabilities of million acre wildfires.

## Parameter estimation {-}

We used a combination of variational approximations and Hamiltonian Monte Carlo methods to sample from the posterior distributions of count and burn area models. 
A variational approximation [@kucukelbir2015automatic] was used for count models to quickly identify the best count model and avoid excessive multi-day model runs. 
Models were fit in the Stan probabilistic programming language using the \code{rstan} package [@carpenter2016stan]. 
The best performing count model and all burn area models were fit using the No-U-Turn Sampler [@hoffman2014no].
We ran four chains for 1000 iterations each, and discarded the first 500 iterations as warmup. 
Convergence was assessed using visual inspection of trace plots, with $\hat{R} \geq 1.1$ as an indicator convergence failure [@brooks1998general].

# Results {-}

## Wildfire occurrence {-}

The zero-inflated negative binomial distribution performed best on the held-out test set (Table \@ref(tab:count-loglik)), and was able to recover the proportion of zeros, count maxima, and count totals in posterior predictive checks for both the training and test data (Figure \ref{ppc-counts}). 
All of the other count models that we considered exhibited lack of fit to at least one of these statistics in posterior predictive checks. 
Hereafter, we report results from the zero-inflated negative binomial model.

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/ppc-counts.png}
\caption{Count predictive checks. Row one shows empirical frequencies of counts as black points, with lines representing predictions. Row two shows the predicted proportion of zeros in the training and test data, with empirical proportions as dashed lines. The third and fourth rows show predicted and empirical maximum and total counts, with empirical values as dashed lines.}
\label{ppc-counts} 
\end{figure}


```{r count-loglik, message=FALSE} 
count_loglik <- read_csv('data/processed/count-loglik.csv')
kable(count_loglik, 
      caption = 'Performance of count models on the test set in descending order. Numbers represent posterior means with standard deviations in parentheses for the test set log likelihood.') %>%
  kable_styling(latex_options = 'striped')
```


```{r rho-beta, message = FALSE}
rho_beta <- read_csv('data/processed/rho_beta.csv') %>% unlist
```

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/count-partial-effs.png}
\caption{Partial effects on the log-transformed negative binomial mean for each level 3 ecoregion, colored by level 1 ecoregion. Lines are posterior medians and ribbons are 95\% credible intervals.}
\label{count-partial-effs}
\end{figure}

Minimum relative humidity and maximum air temperature had the strongest effects on the negative binomial mean (Figure \ref{count-partial-effs}). 
Results for the zero-inflation component are similar (posterior median for $\rho$: `r round(median(rho_beta), 3)`, 95\% credible interval (CI): `r round(quantile(rho_beta, .025), 3)`, `r round(quantile(rho_beta, .975), 3)`). 
The model uncovered unique effects of meteorological variables at level 1, 2, and 3 ecoregions (Figure \ref{all-coefs}).
For example, the positive interaction effect between the second air temperature basis vector and the L1 Great Plains ecoregions indicates that the expected number of wildfires in plains ecoregions with cold conditions is high relative to other ecoregions. 
The Ozark/Ouachita-Appalachian forest and Ozark Highlands were also identified as having region-specific temperature effects (Figure \ref{all-coefs}). 
These region specific effects manifest as clusters of lines in Figure \ref{count-partial-effs}. 
Housing density showed a unimodal relationship to expected count (Figure \ref{count-partial-effs}), with lower expected counts in unpopulated and heavily populated ecoregions, and higher expected counts with moderately populated ecoregions. 

\begin{figure}[ht] 
\includegraphics[width=.9\textwidth]{fig/all-coefs.png}
\caption{Caterpillar plots of zero inflated negative binomial model coefficients, $\beta^{(\mu)}$ (left) and $\beta^{(\pi)}$ (right). Horizontal line segments denote 95\% credible intervals. Grey segments indicate coefficients with a less that 87\% posterior probability of being positive or negative, and colored segments indicate coefficients that are probably positive (red) or negative (blue). B-spline vectors are indicated by colons, e.g., \code{Humidity:1} indicates the first basis vector corresponding to humidity. Interactions between variables \code{a} and \code{b} are represented as \code{Intxn(a x b)}. Level 1 ecoregions are represented by \code{L1 ecoregion name}, and \code{L2} and \code{L3} indicate level 2 and 3 ecoregions.}
\label{all-coefs} 
\end{figure}

```{r count-interval-coverage, message = FALSE, warning=FALSE}
count_test_intervals <- read_csv('data/processed/count_test_intervals.csv')

ecoregion_count_coverage <- count_test_intervals %>%
   group_by(NA_L3NAME) %>%
   summarize(interval_coverage = mean(in_interval)) %>%
   arrange(interval_coverage)

ecoregion_count_worst <- ecoregion_count_coverage %>%
  filter(interval_coverage == min(interval_coverage))

count_out_of_interval <- count_test_intervals %>%
  filter(!in_interval) %>%
  summarize(more_than_predicted = mean(n_fire > hi), 
            less_than_predicted = mean(n_fire < lo), 
            max_diff = max(n_fire - hi))

worst_out_of_interval <- count_test_intervals %>%
  mutate(diff = n_fire - hi) %>%
  filter(diff == max(diff))

area_df <- read_csv('data/processed/area_df.csv')

total_count_coverage <- count_test_intervals %>%
  group_by(NA_L3NAME) %>%
  summarize(interval_coverage = mean(in_interval)) %>%
  left_join(area_df) %>%
  ungroup %>%
  summarize(p_100_pct = mean(interval_coverage == 1),
            n_100_pct = sum(interval_coverage == 1),
            pct_area_100_pct = sum(area[interval_coverage == 1]) / sum(area))
```

Posterior 95\% credible interval coverage for the number of fires over 1000 acres in the test set was `r round(100 * mean(count_test_intervals$in_interval), 1)`\%. 
The lowest test set interval coverage was `r round(100 * ecoregion_count_worst$interval_coverage, 1)`\%, in the `r ecoregion_count_worst$NA_L3NAME` L3 ecoregion.
When observed counts fell outside the 95\% prediction interval, counts were larger than predicted `r 100 * count_out_of_interval$more_than_predicted`\% of the time.
The largest difference between observed numbers and predicted 97.5\% posterior quantiles (the upper limit for the 95\% credible interval) occurred for the `r worst_out_of_interval$NA_L3NAME` L3 ecoregion in `r month.name[worst_out_of_interval$month]` `r worst_out_of_interval$FIRE_YEAR`, when `r worst_out_of_interval$n_fire` fires over 1000 acres occurred and at most `r worst_out_of_interval$hi` were predicted.
For nearly half of the level 3 ecoregions (`r total_count_coverage$n_100_pct` of 85), accounting for `r round(100 * total_count_coverage$pct_area_100_pct, 1)`\% of the land area of the contiguous U.S., the zero-inflated negative binomial model had 100\% test set prediction interval coverage.

## Wildfire burned areas {-}

```{r ba-loglik, message=FALSE} 
burn_area_loglik <- read_csv('data/processed/burn-area-loglik.csv')
kable(burn_area_loglik, 
      caption = 'Performance of burn area models on the test set in descending order. Numbers represent posterior means with standard deviations in parentheses for the test set log likelihood.') %>%
  kable_styling(latex_options = 'striped')
```

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/ppc-density-funs.png}
\caption{Predictive checks for burn area models. The top row shows predicted density in color and empirical density for the training set in black. Row two shows the complementary cumulative distribution function (CCDF) at the tails, with 95\% and 50\% prediction intervals shown in color and observed data as black points. The third and fourth rows show checks for maximum and total burn areas in the training and test set, with observed values as dashed lines and posterior draws as colored points.}
\label{ppc-density-funs}
\end{figure}

The lognormal distribution performed best on the test set (Table \@ref(tab:ba-loglik)), and captured tail-behavior better than other burn area distributions (Figure \ref{ppc-density-funs}).
Although we might have expected the GPD to perform best given its theoretical support for exceedances over a threshold, in this case the GPD model was too heavy-tailed to adequately capture the pattern in the empirical data, predicting fires far larger than those observed in the training and test sets (Figure \ref{ppc-density-funs}). 
The tapered Pareto distribution was too light-tailed (Figure \ref{ppc-density-funs}).
The gamma and Weibull models performed very poorly overall on the test set  (Table \@ref(tab:ba-loglik)), apparently due to a lack of congruence between the shapes of these distributions and the actual burn area distribution. 
Despite a poor fit to the bulk of the wildfire burn area distribution, both of these models performed adequately in the upper tails (Figure \ref{ppc-density-funs}).
Hereafter we present results for the lognormal model, which had the highest test set log likelihood and captured tail behavior of the empirical fire size distribution.

Relative humidity was the primary driver of expected burn area for a fire event (Figure \ref{burn-area-effs}A). 
The first basis vector for mean daily minimum relative humidity was the only coefficient with a 95\% credible interval that did not include zero (posterior median: 1.67, 95\% CI: (0.025, 2.316)). 
This nonlinear effect can be observed in Figure \ref{burn-area-effs}B as a marked increase in the expected burn area below 20\% mean daily minimum humidity. 
This leads to a periodicity gradient among ecoregions in the seasonality of expected fire sizes, with little or no periodic signal in humid ecoregions (Figure \ref{burn-area-effs}C).
There was not strong evidence that meteorological variables had spatially variable effects on expected wildfire burn area. 

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/burn-area-effs.png}
\caption{\textbf{A}. Estimated posterior medians and 95\% credible intervals for each of the 3,473 coefficients associated with expected burn area. Only one coefficient - the first basis vector for humidity - had a 95\% credible interval that excluded zero, shown in red. This effect is visualized in \textbf{B}. Partial effects of mean daily minimum humidity for each level 3 ecoregion, with posterior medians drawn as lines, and the 95\% credible intervals as ribbons. \textbf{C}. Monthly time series of expected fire sizes for every level 3 ecoregion, faceted and colored by level 1 ecoregions sorted by mean humidity. Lines are posterior medians and ribbons are 95\% credible intervals. The year 2010 marks the boundary between the training and test set.}
\label{burn-area-effs} 
\end{figure}

```{r, message = FALSE, results='hide'}
area_coverage <- read_csv('data/processed/area_coverage.csv') %>%
  filter(model == 'ba_lognormal_fit.rds') %>%
  arrange(idx)

holdout_burns <- read_rds('data/processed/holdout_burns.rds') %>%
  as_tibble()

assert_that(all(area_coverage$NA_L3NAME == holdout_burns$NA_L3NAME))

burn_area_coverage <- holdout_burns %>%
  mutate(idx = 1:n()) %>%
  full_join(area_coverage)

worst_area_coverage <- burn_area_coverage %>%
  group_by(NA_L3NAME) %>%
  summarize(coverage = mean(true_in_interval), 
            n = n()) %>%
  arrange(coverage) %>%
  data.frame %>%
  left_join(area_df)

most_worst <- worst_area_coverage %>%
  filter(coverage == min(coverage))
assert_that(nrow(most_worst) == 1)

second_worst <- worst_area_coverage %>%
  anti_join(most_worst) %>%
  filter(coverage == min(coverage))
assert_that(nrow(second_worst) == 1)

smaller_larger_df <- filter(burn_area_coverage, !true_in_interval) %>%
  mutate(real_smaller = true_exceedance < lo, 
         real_larger = true_exceedance > hi) %>%
  ungroup %>%
  summarize(p_smaller = mean(real_smaller), 
            p_larger = mean(real_larger))

biggest_discrepancy <- burn_area_coverage %>%
  filter(!true_in_interval) %>%
  mutate(discrepancy = true_exceedance - hi) %>%
  select(FIRENAME, lo, hi, true_exceedance, discrepancy, 
         NA_L3NAME, ym) %>%
  arrange(-discrepancy) %>%
  filter(discrepancy == max(discrepancy))
```

Overall, 95\% posterior predictive interval coverage in the test set for burn areas was `r 100 * round(mean(burn_area_coverage$true_in_interval), 3)`\%. 
The lowest test set coverage was `r 100 * round(most_worst$coverage)`\%, for the `r most_worst$NA_L3NAME` L3 ecoregion, followed by `r 100 * round(second_worst$coverage, 1)`\%, for the `r second_worst$NA_L3NAME` L3 ecoregion, though these ecoregions had just `r most_worst$n` and `r second_worst$n` wildfire events in the test set. 
When observed fire sizes fell outside the 95\% prediction interval, `r 100 * round(smaller_larger_df$p_smaller, 3)`\% of wildfires were smaller than predicted, and `r 100 * round(1 - smaller_larger_df$p_smaller, 3)`\% of wildfires were larger than predicted. 
The largest discrepancy between the actual size of a wildfire and the predicted 97.5\% posterior quantile was observed with the Wallow Fire in 2011 which burned `r format(round(biggest_discrepancy$true_exceedance) + 1000, big.mark = ',', scientific = FALSE)` acres, but the predicted upper limit for size was `r format(round(biggest_discrepancy$hi) + 1000, big.mark = ',', scientific = FALSE)`.
We investigate this discrepancy further in the case study below.
The lognormal burn area model achieved 100\% interval coverage in `r sum(worst_area_coverage$coverage == 1)` of `r worst_area_coverage %>% nrow` ecoregions that had wildfire events in the test set, accounting for `r 100 * round(sum(worst_area_coverage$area[worst_area_coverage$coverage == 1]) / sum(area_df$area), 3)`\% of the land area of the contiguous US.

## Inference on extremes {-}

```{r, message=FALSE}
mev_intervals <- read_csv('data/processed/mev_intervals.csv') %>%
  filter(!is.na(empirical_max)) %>%
  mutate(in_interval = m_qlo <= empirical_max & m_qhi >= empirical_max) %>%
  ungroup

predicted_totals <- read_csv('data/processed/predicted_totals.csv')

million_acres <- read_csv('data/processed/million-acre-prob.csv')
```

By combining the output of the event count and burn area models, we derived posterior prediction intervals for the size of the largest fire in a month for each region, integrating over uncertainty in the number of fires, as well as the lognormal mean and standard deviation for burn area.
In the holdout period from 2010 to 2015, a 99\% prediction interval achieved `r 100 * round(mean(mev_intervals$in_interval), 3)`\% interval coverage, with `r 100 * round(mean(mev_intervals$empirical_max > mev_intervals$m_qhi), 3)`\% of the block maxima (`r sum(mev_intervals$empirical_max > mev_intervals$m_qhi)` fire events) being larger than predicted (Figure \ref{max-preds-l3}). 
The model predicted the total area burned over the entire test period from 2010 to 2015 to be `r format(median(predicted_totals$predicted_total_area), big.mark = ',', scientific = FALSE)` (95\% CI: (`r format(quantile(predicted_totals$predicted_total_area, .025), big.mark = ',', scientific = FALSE)` - `r format(quantile(predicted_totals$predicted_total_area, .975), big.mark = ',', scientific = FALSE)`) and the actual value was `r format(unique(predicted_totals$actual_total_area), big.mark = ',', scientific = FALSE)`. 
While fires over a million acres in size have happened historically, all fires in the training and test sets were below one million acres. 
If we extrapolate, the probability that at least one fire exceeded one million acres in the period from 2010 to 2015 was estimated to be between `r round(million_acres$lo, 3)` and `r round(million_acres$hi, 3)` (95\% CI), with a posterior median of `r round(million_acres$med, 3)`.

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/max-preds-l3.png}
\caption{Posterior 99\% prediction intervals for the burn area of the largest fire event by month and level 3 ecoregion in the test set, shown for ecoregions with wildfires in more than 20 months. Empirical maxima are shown as black dots.}
\label{max-preds-l3}
\end{figure}

## Error analysis case study: the 2011 Wallow Fire {-}

```{r}
wallow_may_df <- mev_intervals %>%
  filter(NA_L3NAME == 'Arizona/New Mexico Mountains', year == 2011, month == 5)

wallow_jun_df <- mev_intervals %>%
  filter(NA_L3NAME == 'Arizona/New Mexico Mountains', year == 2011, month == 6)
```

To better understand how well the model could or could not anticipate notable extreme events, and why, we used the largest fire in the test set as a case study. 
The Wallow Fire was accidentally ignited on May 29, 2011 by two campers in the L3 Arizona/New Mexico Mountains ecoregion.
It burned through the month of June and into early July. 
The model underpredicted the total burn area of the Wallow Fire. 
The 99\% credible interval for the maximum fire size for May 2011 was (`r round(wallow_may_df$m_qvlo) %>% format(big.mark = ',', scientific = FALSE)`, `r round(wallow_may_df$m_qvhi) %>% format(big.mark = ',', scientific = FALSE)`) acres, but the Wallow Fire is recorded as 563,655 acres in the MTBS data. 
The 99\% credible interval for June 2011 was (`r round(wallow_jun_df$m_qvlo) %>% format(big.mark = ',', scientific = FALSE)`, `r round(wallow_jun_df$m_qvhi) %>% format(big.mark = ',', scientific = FALSE)`) acres, which contains the true value. 

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/attribution-plot.png}
\caption{Posterior median contribution of each input variable to the linear predictor function of model components for the Arizona/New Mexico Mountains level 3 ecoregion from 2010-2016. A dotted vertical line marks May 2011, when the Wallow Fire ignited. Vertical positions of colored lines show contributions to the linear predictor function of each model component.}
\label{attribution-plot}
\end{figure}

We evaluated the contribution of each covariate to the linear predictor functions of the three model components (lognormal mean for burn areas, negative binomial mean for counts, and the logit probability of the zero-inflation component) to understand why these predictions differed. 
We defined the contribution of a variable as the dot product of the elements in the design matrix $\matr{X}$ corresponding to a particular driver variable (e.g., humidity), and the estimated coefficients in $\bm{\beta}$ corresponding to that variable.
This provides a quantitative measure of how each input variable contributes to the linear predictor for an ecoregion, and incorporates the overall, level 1, level 2, and level 3 ecoregion adjustments on these effects. 
Figure \ref{attribution-plot} reveals that humidity is the primary driver of variation in the model's predictions, and that the conditions in June - the month after ignition - favored more large fires, with drier, hotter conditions. 
Evidently, conditions in May that drove (under)predictions of maximum burn area were not representative of the conditions over most of the Wallow Fire's duration.

# Discussion {-}

Extreme wildfires are often devastating, but perhaps they need not be surprising. 
By allowing the non-linear effects of weather and housing density to vary across space, we were able to achieve good predictive accuracy for fire extremes over a five-year prediction window. 
We estimate a non-negligible chance of wildfires larger than what has been observed in recent decades, perhaps even over one million acres.
However, the Wallow Fire case study reveals some promising directions for future prediction efforts. 

Predicting large wildfire occurrence across a large geographic region at a monthly time step could support short to medium term wildfire management and disaster planning. 
However, we have assumed here that high-quality meteorological forecast data are available. 
This is convenient for separating the predictive skill of the fire model from that of a climate model, but in practice predictions are likely to be limited by the quality of climate predictions. 
Weather predictions should be most accurate over relatively short time scales, e.g., upcoming fire seasons, or what will happen next month. 
But, this model could be used in conjunction with longer term climate projections to explore potential wildfire dynamics on a decadal time horizon.

Driving a model with meteorological features raises challenges related to predictive uncertainty and covariate shift - a change in the underlying distribution of forcing variables, potentially outside of the historic range. 
Ideally, this uncertainty would be propagated forward in a predictive model, possibly through stacking of predictive distributions that are generated from multiple models of future climate dynamics [@yao2017using]. 
But, even if one had a perfect forecast, novel conditions present a challenge for predictive modeling [@quionero2009dataset]. 
For example, the High Plains ecoregion had its highest monthly precipitation, lowest 12 month running precipitation, driest, hottest, and windiest conditions in the held-out data, so that the range of environmental conditions in the training data did not encompass the range of future conditions. 
Extrapolating beyond the range of training inputs is generally difficult, but the hierarchical spatial effect specification used here allows partial pooling among climatically similar ecoregions that can inform such predictions, unlike models fit separately to disjoint spatial regions. 

Human-caused climate change is expected to increase fire activity in the western U.S. [@rogers, @Abatzoglou2016] and elsewhere [@flannigan], which when coupled with the nonlinear effect of human-density provides a key inferential wrinkle. 
While most U.S. ecoregions are increasing in human density over time, some of these ecoregions are in the range of values in which this increases the expected number of large fires, while others are so populated that further increases would reduce the chance of a large fire.
The hump-shaped effect of human density on the expected number of large fires is likely driven by ignition pressure and fire suppression [@balch2017human].
As human density increases from zero, ignition pressure increases, but eventually landscapes become so urbanized, fragmented, and/or fire-suppressed that wildfire risk decreases [@Syphard2007; @bowman2011human; @Bistinas2013; @Knorr2013; @Mcwethy2013; @Syphard2017].
At intermediate density, wildfire dynamics respond to human ignition and altered fuel distributions [@Guyette2002], but these responses depend on environmental context and characteristics of the human population [@Marlon2008; @Li2009]. 
This model indicates that the combination of moderate to high human density and dry conditions would nonlinearly increase the chance of an extreme fire event. 
Both human density and dryness are expected to increase in the future across large swaths of the US [@lloyd2017high; @stavros2014regional,  @radeloff2010housing]. 
This result raises questions around how wildfire burn area scales with consequences such as human mortality, health risks from smoke and particulate emission, and the financial burden of wildfire management and disaster recovery. 

This work points to promising directions for future predictive efforts.
Default choices such as Poisson and GPD distributions should be checked against alternatives that may perform better. 
Further, the predictive skill of this model seems to suggest that ordinary events inform the extremes, which would not be the case if the generative distribution of extremes was completely unique. 
Enhancing the spatiotemporal resolution of predictive models could better represent climatic and social drivers of fire dynamics and provide localized insights into fire dynamics to inform decision-making. 
This raises computational challenges, but recent advances in distributed probabilistic computing [@tran2017deep], efficient construction of spatiotemporal point processes [@shirota2018scalable], and compact representations of nonlinear spatial interactions [@lee2011p] may provide solutions. 

The Wallow Fire case study reveals at least one limitation of increasing the spatiotemporal resolution with this approach. 
When the model predictions are driven by covariates that are summarized in space and time (e.g. a mean across an ecoregion in a month), summary values may not represent conditions that are most relevant to an event. 
With a discrete space-time segmentation, events can occur at the boundary of a spatiotemporal unit, e.g., if a fire spreads into an adjacent ecoregion or ignites on the last day of the month. 
Long-burning large wildfires can span months, and a model that only uses conditions upon ignition to predict total burn area can fail to account for conditions that change over the course of the event. 
Modeling ignitions as a point process in continuous space and time [@brillinger2003risk], and explicitly modeling subsequent fire duration and spatial dynamics might identify conditions that ignite fires and those that affect spread. 
Such an approach might be amenable to including information on fuel continuity, which is likely to limit the size of extremely large fires and did not factor into the current models predictions [@rollins2002landscape; @hargrove2000simulating]. 

This paper presents and evaluates a statistical approach to explain and predict extreme wildfires that incorporates spatially varying non-linear dynamics. 
The model reveals considerable differences in fire dynamics among ecoregions spanning the mountain west to the great plains, deserts, and eastern forests, and suggests a decent chance of very large fires exceeding one million acres in the contiguous US. 
Predictive approaches such as this can inform decision-making by placing probabilistic bounds on the number of wildfires and their sizes, while provide deeper insights into wildfire ecology. 

# Acknowledgments {-}

We thank Mitzi Morris, Kyle Foreman, Daniel Simpson, Bob Carpenter, and Andrew Gelman for contributing to the implementation of an intrinsic autoregressive spatial prior in Stan.

\nolinenumbers

# References {-}

<div id="refs"></div>

\newpage



# Supporting Information {-}

\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}

## Prior specifications {-}

Prior distributions were chosen to regularize coefficients on the distribution specific means $\beta^{(\mu)}$ and structural zero parameters $\beta^{(\pi)}$. 
We used a regularized horseshoe prior on these coefficients, which shrinks irrelevant coefficients towards zero, while regularizing nonzero coefficients [@piironen2017sparsity]. 
For zero-inflated models, we used a multivariate version of the regularized horseshoe [@peltola2014hierarchical]:

$$\begin{pmatrix}
           \beta^{(\mu)}_j \\
           \beta^{(\pi)}_j
         \end{pmatrix} 
\sim 
\textnormal{N}
\Bigg(
\bm 0, 
\begin{pmatrix} 
  \tau^2_1 \tilde{\lambda}^2_{1, j} & 
    \rho \tau_1 \tau_2 \tilde{\lambda}_{1, j} \tilde{\lambda}_{2, j} \\
  \rho \tau_1 \tau_2 \tilde{\lambda}_{1, j} \tilde{\lambda}_{2, j} & 
    \tau^2_2 \tilde{\lambda}^2_{2, j}
\end{pmatrix}
\Bigg),$$

$$\tilde{\lambda}^2_{m, j} = \dfrac{c_m^2 \lambda_{j}^2}{c_m^2 + \tau_m^2 \lambda_{j}^2},$$

for each response dimension $m= 1, 2$ and coefficient $j = 1, ..., p$.
Here $\rho$ is a correlation parameter, $\tau_1$ and $\tau_2$ are global variance hyperparameters, $c_1$ and $c_2$ are hyperparameters that determine the amount of shrinkage on the largest coefficients, and $\lambda_{j}$ is a local scale parameter drawn from a half-Cauchy distribution that control the amount of shrinkage applied to coefficient $j$ [@piironen2017sparsity].
With this prior specification, information can be shared across the two response dimensions through the correlation parameter $\rho$, and/or through the local scale parameters $\lambda_j$. 
For count models without structural zeros (the Poisson and negative binomial models), this multivariate prior collapses to a univariate regularized horseshoe prior. 

Spatiotemporal random effects were constructed using a temporally autoregressive, spatially intrinsically autoregressive formulation [@besag1995conditional; @banerjee2014hierarchical]. 
Temporarily suppressing the superscript that indicates whether the effects are on $\mu$ or $\pi$, and denoting column $t$ from an $S \times T$ $\matr{\Phi}$ as $\bm{\phi}_t$ we have:

$$\bm{\phi}_{t=1} \sim \textnormal{N}(\bm{0}, (\tau^{(\phi)}(\matr{D} - \matr{W}))^{-1})$$

$$\bm{\phi}_{t} \sim \textnormal{N}(\eta \bm{\phi}_{t - 1}, (\tau^{(\phi)}(\matr{D} - \matr{W}))^{-1}), \quad t = 2, ..., T$$

where $\eta$ is a temporal dependence parameter, $\tau^{(\phi)}$ is a precision parameter, $\matr D$ is an $S \times S$ diagonal matrix with entries corresponding to the number of spatial neighbors for each spatial unit, and $\matr W$ is an $S \times S$ spatial adjacency matrix with nonzero elements only when spatial unit $i$ is a neighbor of spatial unit $j$ ($w_{i, j} = 1$ if $i$ is a neighbor of $j$, and $w_{i, j} = 0$ otherwise, including $w_{i, i} = 0$ for all $i$). 
$\tau^{(\phi)}$ is a precision parameter. 
We imposed a soft identifiability constraint that places high prior mass near $\sum_{s = 1}^S \phi^*_{t, s} = 0$ for all $t$.

We applied a univariate regularized horseshoe prior to all $\beta$ coefficients in burn area models [@piironen2017sparsity]:

$$ \beta_j
\sim 
\textnormal{N}
\big(
0, 
\tau^2 \tilde{\lambda}^2_{j}
\big), 
\quad
\tilde{\lambda}^2_{j} = \dfrac{c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2},$$

Spatiotemporal random effects were constructed in the same way as for the count models.

\newpage

## Joint distributions {-}

Here we provide the unnormalized posterior densities for each model.
Square brackets represent a probability mass or density function. 
Parameterizations for model likelihoods are provided first, followed by the factorization of the joint distribution, with explicit priors.


### Poisson wildfire count model {-}

$[n | \mu] = \dfrac{\mu^ne^{-\mu}}{n!},$ where $\begin{array} {l} \mu: \textnormal{mean} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\phi}, \sigma^{(\phi)}, \eta, \bm{\lambda}, c, \tau \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \phi_{s, t}] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)} | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\sigma^{(\phi)}] [\eta] [c] [\tau] [\alpha^{(\mu)}] \\
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{Poisson}(n_{s, t} | \textnormal{exp}(\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi_{s, t})) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j^{(\mu)} | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \\ 
&& \textnormal{Normal}^+(\tau | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2).
\end{flalign*}

\newpage

### Negative binomial wildfire count model {-}

$[n | \mu, \delta] = \binom{n + \delta - 1}{n} \big( \frac{\mu}{\mu + \delta} \big)^n \big( \frac{\delta}{\mu + \delta} \big)^\delta$, where $\begin{array}{l} \mu: \textnormal{mean} \\ \delta: \textnormal{dispersion} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\phi}, \sigma^{(\phi)}, \eta, \bm{\lambda}, c, \tau, \delta \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \phi_{s, t}, \delta] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)} | \lambda_j, c, \tau] [\lambda_j] \times \\ 
&& [\sigma^{(\phi)}] [\eta] [c][\tau] [\alpha^{(\mu)}] [\delta] \\
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{Negative Binomial}(n_{s, t} | \textnormal{exp}(\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi_{s, t}), \delta) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j^{(\mu)} | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \\ 
&& \textnormal{Normal}^+(\tau | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2) \times \textnormal{Normal}^+(\delta | 0, 5^2).
\end{flalign*}

\newpage

### Zero-inflated Poisson wildfire count model {-}

$[n | \mu, \pi] = I_{n=0} (1-\pi + \pi e ^{-\mu}) + I_{n > 0} \pi \frac{\mu^ne^{-\mu}}{n!}$, where $\begin{array} {l} \mu: \textnormal{Poisson mean} \\ \pi: 1 - \textnormal{Pr(extra 0)} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, 
 \bm{\phi}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}, 
 \bm{\phi}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}, 
 \bm{\lambda}, c, \tau, \rho \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, \phi_{s, t}^{(\mu)}, \phi_{s, t}^{(\pi)}] \times \\
&& [\bm{\phi}_1^{(\mu)} | \sigma^{(\phi, \mu)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\mu)} | \bm{\phi}_{t - 1}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}] \times \\
&& [\bm{\phi}_1^{(\pi)} | \sigma^{(\phi, \pi)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\pi)} | \bm{\phi}_{t - 1}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)}, \beta_j^{(\pi)} | \lambda_j, c, \tau, \rho] [\lambda_j] \times \\
&& [\sigma^{(\phi, \mu)}] [\sigma^{(\phi, \pi)}] [\eta^{(\mu)}] [\eta^{(\pi)}]
[\alpha^{(\mu)}] [\alpha^{(\pi)}] [\rho] \prod_{m = 1}^2 [c_m] [\tau_m] \\
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{ZIP}(n_{s, t} | e^{\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi^{(\mu)}_{s, t}}, \textnormal{logit}^{-1}(\alpha^{(\pi)} + \matr{X}_{(s, t)} \bm{\beta}^{(\pi)} + \phi^{(\pi)}_{s, t})) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\mu)}_1 | \bm{0}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\mu)}_{t} | \eta^{(\mu)} \bm{\phi}^{(\mu)}_{t - 1}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\pi)}_1 | \bm{0}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\pi)}_{t} | \eta^{(\pi)} \bm{\phi}^{(\pi)}_{t - 1}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{N}
\Bigg(\begin{pmatrix}
           \beta^{(\mu)}_j \\
           \beta^{(\pi)}_j
         \end{pmatrix} \Big{|} \,
\bm 0, 
\begin{pmatrix} 
  \tau^2_1 \frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2} & 
    \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} \\
  \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} & 
    \tau^2_2 \frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}
\end{pmatrix}
\Bigg) \times \\
&& \prod_{j = 1}^p \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&&\textnormal{Normal}^+(\sigma^{(\phi, \mu)} | 0, 1^2) \times \textnormal{Normal}^+(\sigma^{(\phi, \pi)} | 0, 1^2) \times \\
&& \textnormal{Beta}(\eta^{(\mu)} | 1, 1) \times \textnormal{Beta}(\eta^{(\pi)} | 1, 1) \times \\
&& \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\pi)} | 0, 5^2) \times \textnormal{LKJ}(\rho | 3) \times \\
&& \prod_{m = 1}^2 \textnormal{Inv-Gamma}(c_m^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau_m | 0, 5^2).
\end{flalign*}

\newpage 

### Zero-inflated negative binomial wildfire count model {-}

$[n|\mu, \delta, \pi] = I_{n=0} (1-\pi + \pi \big( \frac{\delta}{\mu + \delta} \big)^\delta) + I_{n > 0} \binom{n + \delta - 1}{n} \big( \frac{\mu}{\mu + \delta} \big)^n \big( \frac{\delta}{\mu + \delta} \big)^\delta$, where $\begin{array} {l} \mu: \textnormal{NB mean} \\ \delta: \textnormal{NB dispersion} \\ \pi: 1 - \textnormal{Pr(extra 0)} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, 
 \bm{\phi}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}, 
 \bm{\phi}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}, 
 \bm{\lambda}, c, \tau, \rho, \delta \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, \phi_{s, t}^{(\mu)}, \phi_{s, t}^{(\pi)}, \delta] \times \\
&& [\bm{\phi}_1^{(\mu)} | \sigma^{(\phi, \mu)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\mu)} | \bm{\phi}_{t - 1}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}] \times \\
&& [\bm{\phi}_1^{(\pi)} | \sigma^{(\phi, \pi)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\pi)} | \bm{\phi}_{t - 1}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)}, \beta_j^{(\pi)} | \lambda_j, c, \tau, \rho] [\lambda_j] \times \\
&& [\sigma^{(\phi, \mu)}] [\sigma^{(\phi, \pi)}] [\eta^{(\mu)}] [\eta^{(\pi)}]
[\alpha^{(\mu)}] [\alpha^{(\pi)}] [\rho] [\delta] \prod_{m = 1}^2 [c_m] [\tau_m].
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{ZINB}(n_{s, t} | e^{\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi^{(\mu)}_{s, t}}, \delta, \textnormal{logit}^{-1}(\alpha^{(\pi)} + \matr{X}_{(s, t)} \bm{\beta}^{(\pi)} + \phi^{(\pi)}_{s, t})) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\mu)}_1 | \bm{0}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\mu)}_{t} | \eta^{(\mu)} \bm{\phi}^{(\mu)}_{t - 1}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\pi)}_1 | \bm{0}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\pi)}_{t} | \eta^{(\pi)} \bm{\phi}^{(\pi)}_{t - 1}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{N}
\Bigg(\begin{pmatrix}
           \beta^{(\mu)}_j \\
           \beta^{(\pi)}_j
         \end{pmatrix} \Big{|} \,
\bm 0, 
\begin{pmatrix} 
  \tau^2_1 \frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2} & 
    \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} \\
  \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} & 
    \tau^2_2 \frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}
\end{pmatrix}
\Bigg) \times \\
&& \prod_{j = 1}^p \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&&\textnormal{Normal}^+(\sigma^{(\phi, \mu)} | 0, 1^2) \times \textnormal{Normal}^+(\sigma^{(\phi, \pi)} | 0, 1^2) \times \\
&& \textnormal{Beta}(\eta^{(\mu)} | 1, 1) \times \textnormal{Beta}(\eta^{(\pi)} | 1, 1) \times \\
&& \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\pi)} | 0, 5^2) \times \textnormal{LKJ}(\rho | 3)  \times \textnormal{Normal}^+(\delta | 0, 5^2) \times \\
&& \prod_{m = 1}^2 \textnormal{Inv-Gamma}(c_m^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau_m | 0, 5^2).
\end{flalign*}

\newpage 

### Generalized Pareto/Lomax burn area model {-}

$[y|\sigma, \kappa] = \dfrac{1}{\sigma} \Big( \dfrac{\kappa y}{\sigma} + 1 \Big) ^ {- (\kappa + 1)\kappa^{-1}}$, where $\begin{array} {l} \kappa: \textnormal{shape} \\ \sigma: \textnormal{scale} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \kappa^{(L)}, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \bm{\beta}, \alpha, \phi_{s_i, t_i}, \kappa^{(L)}]  \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\ 
&& [\alpha] [c] [\tau] [\kappa^{(L)}] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&&= \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Lomax}(y_i | \kappa^{(L)}, e^{\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}}) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \\
&& \textnormal{Normal}^+(\kappa^{(L)} | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

### Tapered Pareto burn area model {-}

$[y|\kappa, \nu] = \Big( \dfrac{\kappa}{y} + \dfrac{1}{\nu} \Big) \textnormal{exp} (-x / \nu)$, where $\begin{array}{l} \kappa: \textnormal{shape} \\ \nu: \textnormal{taper} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \nu, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \bm{\beta}, \alpha, \phi_{s_i, t_i}, \nu] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\nu] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&&= \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Tapered Pareto}(y_i | e^{\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}}, \nu) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Cauchy}^+(\nu | 0, 1) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage


### Lognormal burn area model {-}

$[y | \mu, \sigma] = \dfrac{1}{y} \dfrac{1}{\sigma \sqrt[]{2 \pi}} \textnormal{exp}\Big( -\dfrac{(\log (y) - \mu)^2}{2 \sigma^2} \Big)$, where $\begin{array} {l} \mu: \textnormal{location} \\ \sigma: \textnormal{scale} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \sigma, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \beta, \alpha, \phi_{s_i, t_i}, \sigma] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\sigma] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&&= \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Lognormal}(y_i | \alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}, \sigma) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Normal}^+(\sigma | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

### Gamma burn area model {-}

$[y | \kappa, \sigma] = \dfrac{1}{\Gamma (\kappa) \sigma^\kappa} y^{\kappa - 1} \textnormal{exp}(-y / \sigma)$, where $\begin{array} {l} \kappa: \textnormal{shape} \\ \sigma: \textnormal{scale} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \kappa, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \beta, \alpha, \phi_{s_i, t_i}, \kappa] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\kappa] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&& = \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Gamma}(y_i | \kappa, \kappa / \textnormal{exp}(\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i})) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Normal}^+(\kappa | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

### Weibull burn area model {-}

$[y | \kappa, \sigma] = \dfrac{\kappa}{\sigma} \Big( \dfrac{y}{\sigma} \Big)^{\kappa - 1} \textnormal{exp} \Big( - \Big(\dfrac{y}{\sigma} \Big)^\alpha \Big)$, where $\begin{array} {l} \kappa: \textnormal{shape} \\ \sigma: \textnormal{scale} \end{array}$

Unnormalized posterior density:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \kappa, \lambda, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \beta, \alpha, \phi_{s_i, t_i}, \kappa] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\kappa] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&& = \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Weibull}(y_i | \kappa, \textnormal{exp}(\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i})) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Normal}^+(\kappa | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

