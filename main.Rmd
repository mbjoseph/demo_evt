---
title: "Spatiotemporal prediction of wildfire extremes with Bayesian finite sample maxima"
bibliography: library.bib
output:
    bookdown::pdf_document2:
      keep_tex: true
      toc: no
      includes:
          in_header: header.sty
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(knitr)
library(kableExtra)
library(assertthat)

theme_set(theme_minimal() + 
            theme(panel.grid.minor = element_blank()))
```

```{r load-count-test-intervals, message=FALSE, warning=FALSE, results='hide'}
count_test_intervals <- read_csv('data/processed/count_test_intervals.csv')

area_coverage <- read_csv('data/processed/area_coverage.csv') %>%
  filter(model == 'ba_lognormal_fit.rds') %>%
  arrange(idx)

holdout_burns <- read_rds('data/processed/holdout_burns.rds') %>%
  as_tibble()

assert_that(all(area_coverage$NA_L3NAME == holdout_burns$NA_L3NAME))

burn_area_coverage <- holdout_burns %>%
  mutate(idx = 1:n()) %>%
  full_join(area_coverage)

hectares_per_acre <- 0.404686
```

\begin{abstract}
Wildfires are becoming more frequent in parts of the globe, but predicting where and when wildfires occur remains difficult. 
To predict wildfire extremes across the contiguous United States, we integrate a 30 year wildfire record with meteorological and housing data in spatiotemporal Bayesian statistical models with spatially varying nonlinear effects. 
We compared different distributions for the number and sizes of large fires to generate a posterior predictive distribution based on finite sample maxima for extreme events (the largest fires over bounded spatiotemporal domains). 
A zero-inflated negative binomial model for fire counts and a lognormal model for burned areas provided the best performance.
This model attains `r 100 * round(mean(count_test_intervals$in_interval), 2)`\% interval coverage for the number of fires and `r 100 * round(mean(burn_area_coverage$true_in_interval), 2)`\% coverage for fire sizes over a five-year withheld data set. 
Dryness and air temperature strongly predict extreme wildfire probabilities.
Housing density has a hump-shaped relationship with fire occurrence, with more fires occurring at intermediate housing densities. 
Statistically, these drivers affect the chance of an extreme wildfire in two ways: by altering fire size distributions, and by altering fire frequency, which influences sampling from the tails of fire size distributions. 
We conclude that recent extremes should not be surprising, and that the contiguous United States may be on the verge of experiencing even larger wildfire extremes.

\bigskip \noindent \textbf{Key words}: fire; wildfire; Bayesian; spatiotemporal; extremes; climate.
\end{abstract}

\linenumbers


# Introduction {-}

Wildfire frequency and burned area has increased over the past couple decades in the United States [@dennison2014large; @westerling2016increasing], and elsewhere [@krawchuk2009global; @pechony2010driving]. 
In addition to the ecological and smoke impacts associated with increased burned area, there has been an increasing interest in extreme wildfires [@williams2013exploring] given their impact on human lives and infrastructure [@kochi2010economic; @diaz2012economic]. 
While case studies of particular extremes provide insight into what caused past events [@peterson20152013; @nauslar20182017], predictions of future extremes at a national level could inform disaster related resource allocation. 
The term "extreme" has multiple meanings with respect to wildfires [@tedim2018defining], and in this paper we take consider an extreme wildfire to be a fire with the largest burned area over a bounded spatiotemporal domain, i.e., the block maximum within a spatial region and a temporal interval [@coles2001introduction]. 
For example, the block maxima for widlfires across the contiguous U.S. can be defined on a yearly basis (Figure \ref{max-plot}).

```{r mtbs-time-series, results='hide'}
mtbs <- read_rds('data/processed/mtbs.rds') %>%
  mutate(train = FIRE_YEAR < 2010)

annual_maxima <- mtbs %>%
  group_by(FIRE_YEAR) %>%
  mutate(max_size = max(Acres), 
         max_hectares = max_size * hectares_per_acre) %>%
  filter(Acres == max_size)

assert_that(nrow(annual_maxima) == length(unique(mtbs$FIRE_YEAR)))

max_plot <- annual_maxima %>%
  ggplot(aes(Ig_Date, max_hectares)) + 
  geom_point(data = mtbs, aes(y = Acres * hectares_per_acre), alpha = .01) +
  geom_point() + 
  scale_y_log10() + 
  xlab('Ignition date') + 
  ylab('Fire size (hectares)') + 
  scale_x_date(date_breaks = '5 years', date_labels = "%Y")
ggsave('fig/max-plot.png', plot = max_plot, width = 4.5, height = 2)
```

Factors driving wildfire extremes vary in space and time [@barbero2014modeling], but it is unclear how best to account for this in a predictive model. 
Previous efforts have used year or region-specific models, aggregating over space or time [@bermudez2009spatial], temporally or spatially explicit models [@mendes2010spatial], and spatial models with year as a covariate [@diaz2016modeling]. 
Recently, rich spatiotemporal models have been described with linear, spatially constant covariate effects [@serra2014spatio; @serra2014spatioB]. 
However, linear, spatially constant effects are suboptimal over large spatial domains with nonlinear drivers [@fosberg1978weather, @goodrick2002modification, @preisler2004probability; @preisler2007statistical; @balshi2009assessing; @krawchuk2009global; @pechony2009fire; @vilar2010model; @woolford2011spatio; @woolford2014lightning]. 
For example, global wildfire probability shows a hump-shaped relationship with temperature and moisture [@moritz2012climate]. 
Interactions among drivers also impose nonlinearity, e.g., in hot and dry climates fires are fuel limited [@mclaughlin1982effects], but in cold and wet climates fires are energy limited [@Krawchuk2011]. 

Prediction is also complicated by uncertainty in which distribution(s) to use to assign probabilities to extreme events. 
The generalized Pareto distribution (GPD) has frequently been used [@bermudez2009spatial; @jiang2011extreme], but the GPD requires a threshold to delineate extreme events [@davison1990models, @coles2014introduction]. 
The utility and validity of a threshold for extremes in a heterogeneous region is debatable [@tedim2018defining]. 
Recently proposed metastatistical extreme value (MEV) approaches do not require such a threshold, and are based on the statistical distribution of finite sample maxima, i.e., the probability distribution of the maximum value for a finite number of events [@marani2015metastatistical; @zorzetto2016emergence]. 
In the MEV framework, the occurrence and size of future events, and the parameters of their distributions are treated as random variables which together imply a distribution for extremes. 
This approach has roots in compound distributions [@dubey1970compound; @wiitala1999assessing], doubly stochastic processes [@cox1980point], superstatistics [@beck2003superstatistics], and the Bayesian posterior predictive distribution [@gelman2013bayesian]. 
The link to Bayesian inference is particularly useful, as it provides an easy way to propagate uncertainty forward to to predictions of extremes [@coles2003fully].

Here, we extend the finite sample maximum approach to account for non-linear, spatially varying covariate effects with the goal of predicting and explaining extreme wildfire events from a statistical perspective across the contiguous United States.
We aim to predict occurrence (where and when), and magnitude (burned area) of large wildfires at a monthly time scale and regional spatial scale across the conterminous United States. 
Such predictions can be used to prioritize reactive fire suppression resources or inform proactive wildfire risk mitigation.

# Methods {-}

## Data description {-}

```{r read-count-data, message=FALSE, warning=FALSE}
train_counts <- read_rds('data/processed/train_counts.rds')
holdout_counts <- read_rds('data/processed/holdout_counts.rds')
count_df <- full_join(train_counts, holdout_counts)
n_fires_total <- sum(count_df$n_fire)
```

We acquired wildfire event data for the contiguous United States from the Monitoring Trends in Burn Severity (MTBS, www.mtbs.gov) program [@eidenshink20071145801], which includes spatiotemporal information on the occurrence of wildfires in the United States from 1984 to 2016. 
The MTBS data contain fires greater than 1000 acres ($\approx$ 405 hectares) in the western U.S. and greater than 500 acres ($\approx$ 202 hectares) in the eastern U.S. 
For consistency across the U.S., we discarded all records in the MTBS data less than 1000 acres, retaining `r format(n_fires_total, big.mark = ',', scientific = FALSE)` fire events (Figure \ref{study-region}A). 
Each event in the MTBS data has a discovery date, spatial point location, and final size.

To explain fire size and occurrence, we used a combination of meteorological variables including humidity, air temperature, precipitation, and wind speed. 
These variables were selected on the basis of previous work, and also with an aim to drive a predictive model with  interpretable meteorological quantities. 
Meteorological layers were acquired from the gridMET data [@abatzoglou2013development] that blends monthly high-spatial resolution (~4-km) climate data from the Parameter-elevation Relationships on Independent Slopes Model [@daly2008physiographically] with high-temporal resolution (hourly) data from the National Land Data Assimilation System (NLDAS2) using climatologically aided interpolation. 
The resultant products are a suite of surface meteorological variables summarized at the daily time step and at a 4-km pixel resolution. 
Daily total precipitation, minimum relative humidity, mean wind speed, and maximum air temperature were averaged at a monthly time step for each of 84 Environmental Protection Agency level 3 (L3) ecoregions for each month from 1984 to 2016 [@omernik1987ecoregions; @omernik2014ecoregions].
We also computed cumulative monthly precipitation over the previous 12 months for each ecoregion-month combination. 
We chose to segment the U.S. with level 3 ecoregions as a compromise between the more numerous (computationally demanding) level 4 ecoregions, and the coarser level 2 ecoregions. 

We used publicly available housing density estimates that were generated based on the U.S. 2000 decennial census as explanatory variables that may relate to human ignition pressure [@radeloff2010housing]. 
These are provided at decadal time steps, and spatially at the level of census partial block groups. 
To generate approximate measures of housing density at monthly time intervals, we used a simple linear interpolation over time for each block group, then aggregated spatially across block groups to compute mean housing density for each ecoregion in each month. 

## Model development {-}

We built two types of models: one describing the occurrence of fires within each L3 ecoregion over time (i.e., the total number of fires occurring in each ecoregion for each month from 1984 - 2016), and another describing the size of each wildfire in each ecoregion and month. 
For occurrence models, the response variable was a count (number of fires), and for burned area models, the response was a continuous positive quantity (size of each fire event). 
We used the period from 1984 to 2009 for training, witholding the period from 2010 to 2016 to evaluate predictive performance. 

### Fire occurrence {-}

We constructed four models for fire occurrence and compared their predictive performance based on test-set log likelihood and posterior predictive checks for the proportion of zeros, maximum count, and total count. 
The models differed in the distributions used in the likelihood, representing counts as a Poisson, negative binomial, zero-inflated Poisson, or zero-inflated negative binomial random variable. 
The Poisson distribution is a common choice for counts, and the negative binomial distribution provides an alternative that can account for overdispersion. 
The zero-inflated versions of these distributions include a component to represent extra zeros, which might be expected to work well if there are independent processes that determine whether nonzero counts are possible [@lambert1992zero].

For spatial units (ecoregions) $s=1, ..., S$ and time steps (months) $t = 1, ..., T$, each model defines a probability mass function for $n_{s, t}$: the number of fires over 405 hectares in ecoregion $s$ and time step $t$. 
For each of the four count distributions under consideration, location parameters $\mu_{s, t}$ and (for zero-inflated models) structural zero inflation parameters $\pi_{s, t}$ were allowed to vary in space and time.
We used a log link function to ensure that $\mu_{s, t} > 0$, and a logit link function to ensure that $\pi_{s, t} \in (0, 1)$.
Concatenating over spatial and temporal units, so that $\bm{\mu} = (\mu_{s=1, t=1}, \mu_{s=2, t=1}, ..., \mu_{s=S, t=1}, \mu_{s=S, t=2}, ..., \mu_{s=S, t=T})$, and similarly for $\bm{\pi}$, we modeled distributional location and (when applicable) zero inflation parameters as: 

$$\log(\bm{\mu}) = \alpha^{(\mu)} + \matr{X} \bm{\beta}^{(\mu)} + \bm \phi^{(\mu)} + \log (\bm a),$$

$$\textnormal{logit}(\bm{\pi}) = \alpha^{(\pi)} + \matr{X} \bm{\beta}^{(\pi)}  + \bm \phi^{(\pi)},$$

where $\alpha^{(\mu)}$ and $\alpha^{(\pi)}$ are scalar intercept parameters, $\matr{X}$ is a known $(S \times T) \times p$ design matrix, where $p$ is the number of input features, $\bm{\beta}^{(\mu)}$ and $\bm{\beta}^{(\pi)}$ are column vector parameters of length $p$, $\bm \phi^{(\mu)}$  and $\bm \phi^{(\pi)}$ are column vector parameters of length $S \times T$ containing spatiotemporal adjustments, and $\bm a$ is a known offset vector of areas for spatial unit $s = 1, 2, ..., S$, repeated $T$ times.

### Burned area {-}

We developed five candidate models for fire size, each of which specified a different distribution for the size (burned area) of individual fire events [@reed2002power; @hernandez2015statistical], including the generalized Pareto [@hosking1987parameter], tapered Pareto [@schoenberg2003distribution], lognormal, gamma, and Weibull distributions. 
We evaluated each model in terms of test set log likelihood, and posterior predictive checks for fire size extremes. 
We defined the response $y_{i}$ as the number of hectares burned over 405 for the $i^{th}$ fire event, which occurred in spatial unit $s_i$ and time step $t_i$.

Because each burned area distribution has a different parameterization, we included covariate effects in a distribution-specific way. 
For the generalized Pareto distribution (GPD), we assumed a positive shape parameter, leading to a Lomax distribution for exceedances [@bermudez2009spatial]. 
The GPD and Lomax shape parameters are related by $\kappa^{(GPD)}=1 / \kappa^{(L)}$, and the GPD scale parameter is related to the Lomax scale and shape parameters by $\sigma^{(GPD)} = \sigma^{(L)} / \kappa^{(L)}$. 
We introduced covariate dependence via the Lomax scale parameter using a log link. 
For event $i$, $\log(\sigma^{(L)}_{i}) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$, where $\alpha$ is an intercept parameter, $\bm{\beta}$ is a length $p$ vector of coefficients,  $\bm{X}_{(s_i, t_i)}$ is a row vector from $\matr{X}$, and $\phi_{s_i, t_i}$ is a spatiotemporal adjustment for $s_i$ and $t_i$. 
For the tapered Pareto model, we modeled the shape parameter as $\log(\kappa_i) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$. 
The lognormal model included covariate dependence via the location parameter: $\mu_i = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$. 
The gamma model used a log link for the expected value: $\log (E(y_i)) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$. 
Last, we modeled the Weibull scale parameter as $\log(\sigma_i) = \alpha + \bm{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}$.
More detail on the parameterization of each burned area distribution is provided in the Appendices. 


### Accounting for nonlinear forcing {-}

The design matrix $\matr{X}$ was constructed to allow for spatially varying nonlinear effects of housing density and meteorological drivers. 
We used B-splines to account for nonlinearity (Figure \ref{spline-concept}) and allowed the coefficients for each basis vector to vary spatially [@wood2017]. 
First, we constructed univariate B-splines for log housing density, wind speed, same month precipitation, previous 12 month precipitation, air temperature, and humidity, with five degrees of freedom (including an intercept) for each variable.
This step generated 30 basis vectors (five for each of six variables). 



```{r}
colnamesX <- read_rds('data/processed/colnamesX.rds')
X <- read_rds('data/processed/X.rds')
X_pct_zero <- mean(X == 0)
```

To allow for spatial variation in these nonlinear effects, we added interaction effects between each of the basis vectors and ecoregions [@brezger2006generalized; @kneib2009variable].
The hierarchical nesting of ecoregion designations (Figure \ref{study-region}B-D) lends itself to such interactions. 
Conceptually, coefficients in a level 3 ecoregion may be related to coefficients in the level 2 ecoregion containing the level 3 region, the level 1 ecoregion containing the level 2 region, and a global effect. 
The coefficient associated with a basis vector for any level 3 ecoregion is treated as a sum of a global effect, a level 1 ecoregion adjustment, a level 2 ecoregion adjustment, and a level 3 ecogregion adjustment. 
Thus, for every univariate basis vector, we included interaction effects with ecoregion at each of the three ecoregion levels.
This allows borrowing of information across space (level 3 ecoregions in a level 2 ecoregion are often adjacent), and for regions that are ecologically similar. 
We also included adjustments on the global intercept for each level 1, 2, and 3 ecoregion to account for spatial variation that is unrelated to climate or housing density. 
This specification induces sparsity in $\matr{X}$ that we exploit to increase the efficiency of computing $\bm \mu$ and $\bm \pi$. 
In total, $\matr{X}$ has $p=$ `r format(length(colnamesX), big.mark = ',', scientific = FALSE)` columns, with `r round(100 * X_pct_zero, 0)`\% zero entries.

## Prior specification {-}

To avoid overfitting, we used a regularized horseshoe prior on the coefficients associated with the spatially varying nonlinear effects described above [@piironen2017sparsity]. 
This prior places high probability close to zero, while retaining heavy enough tails that nonzero coefficients are not shrunk too strongly toward zero. 
This is consistent with our prior expectation that most of the coefficients associated with the columns in $\matr{X}$ were close to zero.
For the zero inflated count models, we used a multivariate horseshoe to allow information sharing between the zero inflated and distribution specific location parameters [@peltola2014hierarchical]. 
For the remaining count models and all burned area models, this was a univariate horseshoe prior. 
Spatiotemporal random effects were constructed using a temporally autoregressive, spatially intrinsically autoregressive formulation [@besag1995conditional; @banerjee2014hierarchical]. 
Details of these priors and the resulting joint distributions are provided in the Appendices.

## Posterior predictive inference and extremes {-}

We used the posterior predictive distribution to check each model and make inference on extremes. 
The posterior predictive distribution provides a distribution for replications of observed data ($y^{\textnormal{rep}}$), and predictions of future data [@gelman2013bayesian]. 
Conceptually, for a good model, $y^{\textnormal{rep}}$ should be similar to observed training data $y$, and future predictions should be similar to future data. 
Distributions over both quantities can be obtained by conditioning $y$ and marginalizing over model parameters $\theta$, e.g., $[y^{\textnormal{rep}} | y] = \int [y^{\textnormal{rep}} | \theta] [\theta | y] d\theta$. 

Posterior predictive distributions facilitate model checks that compare predicted and observed test statistics [@gelman1996posterior].
To evaluate whether a model captures tail behavior, one can compare an empirical maximum ($T(y) = \textnormal{max}(y)$) to the predicted distribution of maxima $T(y^{\textnormal{rep}})$. 
We also include predictive checks for the proportion of zero counts, and totals for count and burned area models.
Posterior predictive inference for maxima is similar in spirit to the MEV approach. 
Both obtain a distribution over maxima by marginalizing over unknowns including the number of events, size of each event, and parameters of their distributions [@marani2015metastatistical].
However, a Bayesian approach explicitly conditions on the observed data to obtain a posterior distribution of parameters.

Seeing this connection is useful in the context of including priors and propagating uncertainty to derived parameters.
Concretely, for any ecoregion $s$ and timestep $t$, if we define a particular maximum fire size conditional on a fire having occurred as $z_{s, t}$, and let $Z_{s, t}$ represent the random variable of maximum fire size, then the cumulative distribution function (CDF) for $z_{s, t}$ is given by $\text{Pr}(Z_{s, t} \leq z_{s, t}) = \text{F}(y_{(s, t)})^{n_{s, t}}$, where $\text{F}(y_{(s, t)})$ is the CDF of fire size, and $n_{s, t}$ is the number of wildfire events. 
Thus, $\text{Pr}(Z_{s, t} \leq z_{s, t})$ is the distribution function for the finite sample maximum.
The CDF for $z_{s, t}$ can be inverted to produce a quantile function that permits computation of prediction intervals for maximum fire sizes, conditional on fires having occurred. 
Given a collection of posterior draws from a burned area model that parameterize $\text{F}(y_{(s, t)})$, and a collection of posterior draws of $n_{s, t}^{\textnormal{rep}}$ from the posterior predictive distribution of a wildfire count model, a posterior distribution for the CDF or quantile function of maximum fire size can be generated which combines the two models to facilitate inference on the distribution of extremes. 

## Parameter estimation {-}

We used a combination of variational approximations and Hamiltonian Monte Carlo methods to sample from the posterior distributions of count and burned area models. 
A variational approximation [@kucukelbir2015automatic] was used for count models to quickly identify a preferred model. 
The best performing count model and all burned area models were fit using the No-U-Turn Sampler [@hoffman2014no].
Models were fit in the Stan probabilistic programming language using the \code{rstan} package [@carpenter2016stan; @rstan]. 
We ran four chains for 1000 iterations each, and discarded the first 500 iterations as warmup. 
Convergence was assessed using visual inspection of trace plots, with potential scale reduction statistic values $\hat{R} \geq 1.1$ as an indicator convergence failure [@brooks1998general].

## Implementation {-}

All data processing, model fitting, and visualization were implemented with open source software, primarily in the R programming language [@R], and wrapped in a reproducible workflow via GNU Make and Docker [@Stallman2004; @boettiger2015introduction]. 
Data cleaning and transformation required the R packages assertthat [@assertthat], lubridate [@lubridate], Matrix [@Matrix], pbapply [@pbapply], splines [@splines], tidyverse [@tidyverse], and zoo [@zoo]. 
Spatial data were processed with raster [@raster], rgdal [@rgdal], sf [@sf], and spdep [@spdep].
Finally, we used cowplot [@cowplot], ggrepel [@ggrepel], ggthemes [@ggthemes], patchwork [@patchwork], and RColorBrewer [@rcolorbrewer] for visualization. 
The manuscript was written in R Markdown [@rmarkdown]. 
Analyses were run on an Amazon Web Services m5.2xlarge EC2 instance with four physical cores and 32 GB of RAM, and the whole workflow requires $\approx$ 72 hours. 
All code to reproduce the analysis is available on GitHub at https://github.com/mbjoseph/wildfire-extremes [@max_joseph_2018_1326858].

# Results {-}

## Wildfire occurrence {-}

The zero-inflated negative binomial distribution performed best on the held-out test set (Table \@ref(tab:count-loglik)), and was able to recover the proportion of zeros, count maxima, and count totals in posterior predictive checks for both the training and test data (Figure \ref{ppc-counts}). 
All of the other count models that we considered exhibited lack of fit to at least one of these statistics in posterior predictive checks. 
Hereafter, we report results from the zero-inflated negative binomial model.


```{r rho-beta, message = FALSE}
rho_beta <- read_csv('data/processed/rho_beta.csv') %>% unlist
```

Minimum relative humidity and maximum air temperature had the strongest effects on both the zero-inflation component and the expected value of the negative binomial component (Figure \ref{count-partial-effs}, posterior median for $\rho$: `r round(median(rho_beta), 3)`, 95\% credible interval (CI): `r round(quantile(rho_beta, .025), 3)` - `r round(quantile(rho_beta, .975), 3)`). 
The model uncovered unique effects of meteorological variables at level 1, 2, and 3 ecoregions (Figure \ref{all-coefs}).
For example, a positive interaction effect between the second air temperature basis vector and the L1 Great Plains ecoregions indicates that the expected number of wildfires in plains ecoregions with cold conditions is high relative to other ecoregions. 
The Ozark/Ouachita-Appalachian forest and Ozark Highlands were also identified as having region-specific temperature effects (Figure \ref{all-coefs}). 
Twelve month total precipitation also had region specific effects in the Mississippi Alluvial and Southeast Coastal Plains ecoregion, where it was associated with lower expected fire counts (Figure \ref{all-coefs}).
In contrast, increasing cumulative twelve month precipitation was associated with higher counts in desert ecoregions (Figure \ref{count-partial-effs}). 
Housing density showed a unimodal relationship to expected count (Figure \ref{count-partial-effs}), with lower expected counts in sparsely populated ecoregions, and higher expected counts with moderately populated ecoregions. 


```{r count-interval-coverage, message = FALSE, warning=FALSE}
count_test_intervals <- read_csv('data/processed/count_test_intervals.csv')

ecoregion_count_coverage <- count_test_intervals %>%
   group_by(NA_L3NAME) %>%
   summarize(interval_coverage = mean(in_interval)) %>%
   arrange(interval_coverage)

ecoregion_count_worst <- ecoregion_count_coverage %>%
  filter(interval_coverage == min(interval_coverage))

count_out_of_interval <- count_test_intervals %>%
  filter(!in_interval) %>%
  summarize(more_than_predicted = mean(n_fire > hi), 
            less_than_predicted = mean(n_fire < lo), 
            max_diff = max(n_fire - hi))

worst_out_of_interval <- count_test_intervals %>%
  mutate(diff = n_fire - hi) %>%
  filter(diff == max(diff))

area_df <- read_csv('data/processed/area_df.csv')

total_count_coverage <- count_test_intervals %>%
  group_by(NA_L3NAME) %>%
  summarize(interval_coverage = mean(in_interval)) %>%
  left_join(area_df) %>%
  ungroup %>%
  summarize(p_100_pct = mean(interval_coverage == 1),
            n_100_pct = sum(interval_coverage == 1),
            pct_area_100_pct = sum(area[interval_coverage == 1]) / sum(area))
```

Posterior 95\% credible interval coverage for the number of fires over 405 hectares in the test set was `r round(100 * mean(count_test_intervals$in_interval), 1)`\%. 
The lowest test set interval coverage was `r round(100 * ecoregion_count_worst$interval_coverage, 1)`\%, in the `r ecoregion_count_worst$NA_L3NAME` L3 ecoregion.
When observed counts fell outside the 95\% prediction interval, counts were larger than predicted `r 100 * count_out_of_interval$more_than_predicted`\% of the time.
The largest difference between observed numbers and predicted 97.5\% posterior quantiles (the upper limit for the 95\% credible interval) occurred for the `r worst_out_of_interval$NA_L3NAME` L3 ecoregion in `r month.name[worst_out_of_interval$month]` `r worst_out_of_interval$FIRE_YEAR`, when `r worst_out_of_interval$n_fire` fires over 405 hectares occurred and at most `r worst_out_of_interval$hi` were predicted.
For nearly half of the level 3 ecoregions (`r total_count_coverage$n_100_pct` of 85), accounting for `r round(100 * total_count_coverage$pct_area_100_pct, 1)`\% of the land area of the contiguous U.S., the zero-inflated negative binomial model had 100\% test set prediction interval coverage.

## Wildfire burned areas {-}

The lognormal distribution performed best on the test set (Table \@ref(tab:ba-loglik)), and captured tail-behavior better than other burned area distributions (Figure \ref{ppc-density-funs}).
The GPD model was too heavy-tailed to adequately capture the pattern in the empirical data, predicting fires far larger than those observed in the training and test sets (Figure \ref{ppc-density-funs}). 
The tapered Pareto distribution was too light-tailed (Figure \ref{ppc-density-funs}).
The gamma and Weibull models performed very poorly overall on the test set  (Table \@ref(tab:ba-loglik)), apparently due to a lack of congruence between the shapes of these distributions and the actual burned area distribution. 
Despite a poor fit to the bulk of the wildfire burned area distribution, both performed adequately in the upper tails (Figure \ref{ppc-density-funs}).
Hereafter we present results for the lognormal model, which had the highest test set log likelihood and captured tail behavior of the empirical fire size distribution.

```{r burn-area-beta, message = FALSE, results='hide'}
burn_area_beta <- read_csv('data/processed/burn-area-beta.csv') %>%
  mutate(nonzero = (lo > 0 & hi > 0) | (lo < 0 & hi < 0))
hum_1_effect <- filter(burn_area_beta, nonzero)
assert_that(nrow(hum_1_effect) == 1)
```

Relative humidity was the primary driver of expected burned area for a fire event (Figure \ref{burn-area-effs}A). 
The first basis vector for mean daily minimum relative humidity was the only coefficient with a 95\% credible interval that did not include zero (posterior median: `r hum_1_effect$median %>% round(2)`, 95\% CI: (`r hum_1_effect$lo %>% round(2)` - `r hum_1_effect$hi %>% round(2)`)). 
This nonlinear effect can be observed in Figure \ref{burn-area-effs}B as an increase in the expected burned area below 20\% mean daily minimum humidity. 
This leads to a seasonality gradient among ecoregions of expected fire sizes, with little or no seasonal signal in typically humid ecoregions such as Marine West Coast Forests of the Pacific Northwest, and seasonal oscillations in ecoregions that have periodic fluctuations between dry and humid conditions such as the Temperate Sierras (Figure \ref{burn-area-effs}C).
There was not strong evidence that meteorological variables had spatially variable effects on expected wildfire burned area. 


```{r, message = FALSE, results='hide'}
worst_area_coverage <- burn_area_coverage %>%
  group_by(NA_L3NAME) %>%
  summarize(coverage = mean(true_in_interval), 
            n = n()) %>%
  arrange(coverage) %>%
  data.frame %>%
  left_join(area_df)

most_worst <- worst_area_coverage %>%
  filter(coverage == min(coverage))
assert_that(nrow(most_worst) == 1)

second_worst <- worst_area_coverage %>%
  anti_join(most_worst) %>%
  filter(coverage == min(coverage))
assert_that(nrow(second_worst) == 1)

smaller_larger_df <- filter(burn_area_coverage, !true_in_interval) %>%
  mutate(real_smaller = true_exceedance < lo, 
         real_larger = true_exceedance > hi) %>%
  ungroup %>%
  summarize(p_smaller = mean(real_smaller), 
            p_larger = mean(real_larger))

biggest_discrepancy <- burn_area_coverage %>%
  filter(!true_in_interval) %>%
  mutate(discrepancy = true_exceedance - hi) %>%
  select(Fire_Name, lo, hi, true_exceedance, discrepancy, 
         NA_L3NAME, ym) %>%
  arrange(-discrepancy) %>%
  filter(discrepancy == max(discrepancy))
```

Overall, 95\% posterior predictive interval coverage in the test set for burned areas was `r 100 * round(mean(burn_area_coverage$true_in_interval), 3)`\%. 
The lowest test set coverage was `r 100 * round(most_worst$coverage)`\%, for the `r most_worst$NA_L3NAME` L3 ecoregion, followed by `r 100 * round(second_worst$coverage, 1)`\%, for the `r second_worst$NA_L3NAME` L3 ecoregion, though these ecoregions had just `r most_worst$n` and `r second_worst$n` wildfire events in the test set. 
When observed fire sizes fell outside the 95\% prediction interval, `r 100 * round(smaller_larger_df$p_smaller, 3)`\% of wildfires were smaller than predicted, and `r 100 * round(1 - smaller_larger_df$p_smaller, 3)`\% of wildfires were larger than predicted. 
The largest discrepancy between the actual size of a wildfire and the predicted 97.5\% posterior quantile was observed with the Wallow Fire in 2011 which burned `r format((round(biggest_discrepancy$true_exceedance) + 1000) * hectares_per_acre, big.mark = ',', scientific = FALSE)` hectares, but the predicted upper limit for size was `r format((round(biggest_discrepancy$hi) + 1000) * hectares_per_acre, big.mark = ',', scientific = FALSE)`.
We investigate this discrepancy further in the case study below.
The lognormal burned area model achieved 100\% interval coverage in `r sum(worst_area_coverage$coverage == 1)` of `r worst_area_coverage %>% nrow` ecoregions that had wildfire events in the test set, accounting for `r 100 * round(sum(worst_area_coverage$area[worst_area_coverage$coverage == 1]) / sum(area_df$area), 3)`\% of the land area of the contiguous U.S.

## Inference on extremes {-}

```{r, message = FALSE, warning = FALSE}
mev_intervals <- read_csv('data/processed/mev_intervals.csv') %>%
  filter(!is.na(empirical_max)) %>%
  mutate(in_interval = m_qlo <= empirical_max & m_qhi >= empirical_max) %>%
  ungroup

predicted_totals <- read_csv('data/processed/predicted_totals.csv')

million_acres <- read_csv('data/processed/million-acre-prob.csv')

million_er_mon <- read_csv('data/processed/million-er-mon.csv') %>%
  arrange(-med)
```

By combining the output of the event count and burned area models, we derived posterior prediction intervals for the size of the largest fire in a month for each region (the "burned area maximum"), integrating over uncertainty in the number of fires, as well as the lognormal mean and standard deviation for burned area.
We evaluated the posterior distribution for the quantile function of the n-sample maximum of a lognormal distribution ($\exp(\mu + \sigma \sqrt{2}\text{erf}^{-1}(2 P^{1/n} - 1))$, where $\text{erf}^{-1}$ is the inverse error function, and $P$ is a probability) to generate prediction intervals for maximum fire sizes by month and ecoregion, conditional on one or more fires having occurred. 
In the holdout period from 2010 to 2016, a 99\% prediction interval achieved `r 100 * round(mean(mev_intervals$in_interval), 3)`\% interval coverage, with `r 100 * round(mean(mev_intervals$empirical_max > mev_intervals$m_qhi), 3)`\% of the burned area maxima (`r sum(mev_intervals$empirical_max > mev_intervals$m_qhi)` fire events) being larger than predicted (Figure \ref{max-preds-l3}). 
As an additional check, we used the posterior distribution of the model to predict the total area burned by wildfires in the test set.
The model predicted the total area burned over the entire contiguous United States in test period from 2010 to 2016 to be `r format(median(predicted_totals$predicted_total_area), big.mark = ',', scientific = FALSE)` (95\% CI: (`r format(quantile(predicted_totals$predicted_total_area, .025), big.mark = ',', scientific = FALSE)` - `r format(quantile(predicted_totals$predicted_total_area, .975), big.mark = ',', scientific = FALSE)`) and the actual value was `r format(unique(predicted_totals$actual_total_area), big.mark = ',', scientific = FALSE)`. 

While fires over a million acres ($\approx$ 404,686 hectares) in size have happened historically in the contiguous U.S. [@pernin1971great], no such fires were represented in in the training or test sets. 
If we extrapolate, the probability of at least one fire this large in the period from 2010 to 2016 was estimated to be between `r round(million_acres$lo, 3)` and `r round(million_acres$hi, 3)` (95\% CI), with a posterior median of `r round(million_acres$med, 3)`. 
The highest probability for such an event was `r round(million_er_mon$med[1], 3)` (posterior median), with a 95\% CI of (`r round(million_er_mon$lo[1], 3)`, `r round(million_er_mon$hi[1], 3)`) seen for the `r million_er_mon$NA_L3NAME[1]` ecoregion in `r month.name[million_er_mon$month[1]]` `r million_er_mon$year[1]`. 
The second highest probability was `r round(million_er_mon$med[2], 3)` (posterior median), with a 95\% CI of (`r round(million_er_mon$lo[2], 3)`, `r round(million_er_mon$hi[2], 3)`) seen for the `r million_er_mon$NA_L3NAME[2]` ecoregion in `r month.name[million_er_mon$month[2]]` `r million_er_mon$year[2]`. 
Aggregating spatially, we estimated monthly probabilities of a million acre wildfire. 
These probabilities show seasonal signals corresponding to peak fire seasons, with a shift toward higher and broader peaks beginning in the 21st century (Figure \ref{million-acre-probs}).


## Error analysis case study: the 2011 Wallow Fire {-}

```{r}
wallow_may_df <- mev_intervals %>%
  filter(NA_L3NAME == 'Arizona/New Mexico Mountains', year == 2011, month == 5)

wallow_jun_df <- mev_intervals %>%
  filter(NA_L3NAME == 'Arizona/New Mexico Mountains', year == 2011, month == 6)
```

To better understand how well the model could or could not anticipate notable extreme events, and why, we used the largest fire in the test set as a case study. 
The Wallow Fire was accidentally ignited on May 29, 2011 by two campers in the L3 Arizona/New Mexico Mountains ecoregion.
It burned through the month of June and into early July. 
The model underpredicted the total burned area of the Wallow Fire. 
Integrating over uncertainty in the predicted number of fires and expected fire size, the 99\% credible interval for the maximum fire size for May 2011 was (`r round(wallow_may_df$m_qvlo * hectares_per_acre) %>% format(big.mark = ',', scientific = FALSE)` - `r round(wallow_may_df$m_qvhi * hectares_per_acre) %>% format(big.mark = ',', scientific = FALSE)`) hectares, but the Wallow Fire is recorded as 228,103 hectares. 

We evaluated the contribution of each covariate to the linear predictor functions of the three model components (lognormal mean for burned areas, negative binomial mean for counts, and the logit probability of the zero-inflation component) to understand why these predictions differed. 
We defined the contribution of a variable as the dot product of the elements in the design matrix $\matr{X}$ corresponding to a particular driver variable (e.g., humidity), and the estimated coefficients in $\bm{\beta}$ corresponding to that variable.
This provides a quantitative measure of how each input variable contributes to the linear predictor for an ecoregion, and incorporates the overall, level 1, level 2, and level 3 ecoregion adjustments on these effects. 
Humidity is the primary driver of variation in the model's predictions overall, and June 2011 - the month after ignition - favored more large fires, with drier, hotter conditions (Figure \ref{attribution-plot}). 
The 99\% credible interval for June 2011 was (`r round(wallow_jun_df$m_qvlo * hectares_per_acre) %>% format(big.mark = ',', scientific = FALSE)` - `r round(wallow_jun_df$m_qvhi * hectares_per_acre) %>% format(big.mark = ',', scientific = FALSE)`) hectares, which contains the true value. 
Had the Wallow Fire ignited two days later, the true final size would have been contained in the prediction interval. 
Evidently, conditions in May that drove (under)predictions of maximum burned area were not representative of the conditions over most of the Wallow Fire's duration. 

Temporal mismatch aside, meteorological conditions local to the Wallow Fire differed from the monthly regional means (Figure \ref{wallow-conditions}). 
In particular, wind speeds in the Wallow Fire vicinity exceeded the regional monthly mean values on the date of ignition and in the weeks following ignition. 
Over the majority of the duration of the Wallow Fire (May 29 to July 8), local daily conditions were drier and hotter on average than regional mean monthly conditions in May, which were used to drive the statistical model. 
This local variability is not represented in the regional models developed here. 
The failure of the model to correctly predict the size of the Wallow fire suggests potential avenues for improvement, discussed below. 

# Discussion {-}

Extreme wildfires are often devastating, but perhaps they need not be surprising. 
By allowing the non-linear effects of weather and housing density to vary across space, this model achieves good predictive accuracy for fire extremes over a five-year prediction window. 
This model predicts that extremely large wildfires, perhaps even over one million acres (or 404,686 hectares), have a non-negligible probability of occurrence in the conterminous United States.
Such predictions can support regional wildfire management and probabilistic hazard assessment. 

Driving a model with meteorological features raises challenges related to predictive uncertainty and covariate shift - a change in the underlying distribution of forcing variables, potentially outside of the historic range. 
Ideally, this uncertainty would be propagated forward in a predictive model, possibly through stacking of predictive distributions that are generated from multiple models of future climate dynamics [@yao2017using]. 
But, even if one had a perfect forecast, novel conditions present a challenge for predictive modeling [@quionero2009dataset]. 
For example, the High Plains ecoregion had its highest mean monthly precipitation, lowest 12 month running precipitation, driest, hottest, and windiest conditions in the test set period. 
Extrapolating beyond the range of training inputs is generally difficult, but the hierarchical spatial effect specification used here allows partial pooling among climatically similar ecoregions that can inform such predictions, unlike models fit separately to disjoint spatial regions. 

Similar issues could arise when making predictions for observed but rare meteorological conditions. 
For example, mean daily minimum humidity values over 60% accounted for just `r round(mean(train_counts$rmin > 60) * 100, 2)`\% of the ecoregion-months in the training data, and `r sum(train_counts$n_fire[train_counts$rmin > 60])` fires occurred in such months. 
As a consequence, there is relatively little data that can be used to inform the model for such conditions, and the prior distribution which shrinks coefficients toward zero may dominate the likelihood in the posterior distribution. 
In this case, the posterior distribution for the last basis coefficient for the partial effect of humidity is likely to be close to zero. 
This could explain why the estimated partial effect of humidity on the expected counts was less negative at the upper end of the observed humidity range, although previous work has found similarly nonlinear partial effects [@preisler2004probability]. 
The count model performed extremely well in this range, with `r round(mean(count_test_intervals$in_interval[count_test_intervals$rmin > 60]) * 100, 2)`\% interval coverage for the `r sum(count_test_intervals$rmin > 60)` ecoregion-months with mean daily minimum humidity values greater than 60\% in the withheld test data. 
The model nearly always predicted zero counts with high confidence when conditions were this humid: `r sum(count_test_intervals$hi[count_test_intervals$rmin > 60] == 0)` of `r sum(count_test_intervals$rmin > 60)` predictions made for such conditions were 95\% credible intervals of (0, 0), and the remaining prediction had a posterior median of zero, along with a 95\% credible interval from 0 to 1.
Monotonicity constraints could be incorporated into such a model via monotonic spline bases [@ramsay1988monotone], or an ordered prior distribution for basis coefficients [@brezger2008monotonic]. 
In this case, the count model performs well under humid conditions without monotonicity constraints, and there seems to be little room for performance improvements that might result from such constraints.

Human-caused climate change is expected to increase fire activity in the western U.S. [@rogers2011; @westerling2011climate; @moritz2012climate; @Abatzoglou2016] and elsewhere [@flannigan2009], but the nonlinear effect of human-density could provide additional insight into future expectations. 
While most U.S. ecoregions are increasing in human density over time, some of these ecoregions are in the range of values in which this increases the expected number of large fires, while others are so populated that further increases would reduce the chance of a large fire.
The hump-shaped effect of human density on the expected number of large fires is likely driven by ignition pressure and fire suppression [@balch2017human].
As human density increases from zero, ignition pressure increases, but eventually landscapes become so urbanized, fragmented, and/or fire-suppressed that wildfire risk decreases [@Syphard2007; @bowman2011human; @Bistinas2013; @Knorr2013; @Mcwethy2013; @Syphard2017; @nagy2018human].
At intermediate density, wildfire regimes respond to human ignition and altered fuel distributions [@Guyette2002], but these responses depend on environmental context and characteristics of the human population [@Marlon2008; @Li2009]. 
This model indicates that the combination of moderate to high human density and dry conditions would nonlinearly increase the chance of an extreme fire event. 
Both human density and dryness are expected to increase in the future across large swaths of the U.S. [@lloyd2017high; @stavros2014regional,  @radeloff2010housing], with potential implications for human mortality, health risks from smoke and particulate emission, and the financial burden of wildfire management [@reid2016critical; @radeloff2018rapid]. 

This work points to promising directions for future predictive efforts.
Default choices such as Poisson and GPD distributions should be checked against alternative distributions. 
Further, the predictive skill of this model seems to suggest that ordinary events provide information on extremes, which would not be the case if the generative distribution of extremes was completely unique.
Previous case studies have identified that extremes or anomalies in climatological drivers play a role in the evolution of extreme wildfires [@peterson20152013], but for this work, monthly averages of climatological drivers over fairly large spatial regions were used, which may smooth over anomalous or extreme conditions. 
Enhancing the spatiotemporal resolution of predictive models could better represent climatic and social drivers of fire dynamics and provide localized insights into fire dynamics to inform decision-making. 
This raises computational challenges, but recent advances in distributed probabilistic computing [@tran2017deep], efficient construction of spatiotemporal point processes [@shirota2018scalable], and compact representations of nonlinear spatial interactions [@lee2011p] may provide solutions. 

The Wallow Fire case study reveals at least one limitation of increasing the spatiotemporal resolution. 
When the model predictions are driven by covariates that are summarized in space and time (e.g. a mean across an ecoregion in a month), summary values may not represent conditions that are most relevant to an event. 
With a discrete space-time segmentation, events can occur at the boundary of a spatiotemporal unit, e.g., if a fire spreads into an adjacent ecoregion or ignites on the last day of the month. 
Large wildfires can span months, and a model that only uses conditions upon ignition to predict total burned area can fail to account for conditions that change over the course of the event. 
Modeling ignitions as a point process in continuous space and time [@brillinger2003risk], and explicitly modeling subsequent fire duration and spatial dynamics could better separate conditions that ignite fires from those that affect spread. 
Such an approach might be amenable to including information on fuel continuity, which is likely to limit the size of extremely large fires and did not factor into the current models predictions [@rollins2002landscape; @hargrove2000simulating]. 

To the extent that a model reflects the generative process for extreme events, the decomposition of contributions to the model's predictions may provide insight into attribution for meteorological and anthropogenic drivers of extremes. 
However, a model trained to represent a region-wide distribution of fire sizes will inevitably fail to capture local factors that are relevant to specific events such as the Wallow Fire. 
If predicting the spread dynamics of particular fire events is a goal, process-based models designed to model fire spread are likely to be more appropriate than regional statistical models such as those developed here. 

This paper presents and evaluates a statistical approach to explain and predict extreme wildfires that incorporates spatially varying non-linear effects. 
The model reveals considerable differences in fire dynamics among ecoregions spanning the mountain west to the great plains, deserts, and eastern forests, and suggests a non-negligible chance of extreme wildfires larger than those seen in over the past 30 years in the contiguous U.S. 
Predictive approaches such as this can inform decision-making by placing probabilistic bounds on the number of wildfires and their sizes, while provide deeper insights into wildfire ecology. 

# Acknowledgments {-}

We thank Mitzi Morris, Kyle Foreman, Daniel Simpson, Bob Carpenter, and Andrew Gelman for contributing to the implementation of an intrinsic autoregressive spatial prior in Stan.


# Literature cited {-}

<div id="refs"></div>

\clearpage

# Tables {-}


```{r count-loglik, message=FALSE} 
count_loglik <- read_csv('data/processed/count-loglik.csv')
kable(count_loglik, 
      caption = 'Performance of count models on the test set in descending order. Posterior means are provided with standard deviations in parentheses.', 
      longtable = TRUE) %>%
  kable_styling(latex_options = 'striped')
```

\newpage

```{r ba-loglik, message=FALSE} 
burn_area_loglik <- read_csv('data/processed/burn-area-loglik.csv')
kable(burn_area_loglik, 
      caption = 'Performance of burned area models on the test set in descending order. Posterior means are provided with standard deviations in parentheses.', 
      longtable = TRUE) %>%
  kable_styling(latex_options = 'striped')
```

\clearpage

# Figures {-}

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/max-plot.png}
\caption{Sizes of wildfires over 405 hectares in the contiguous United States, from the Monitoring Trends in Burn Severity multiagency program. Each point represents a fire event, and the largest fires for each year (the block maxima) are shown as solid black points.}
\label{max-plot}
\end{figure}

\clearpage

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/maps.png}
\caption{A. Large wildfire ignition locations are shown as points across the study region. Colors in panels B, C, and D show level 1, 2, and 3 ecoregions respectively.}
\label{study-region}
\end{figure}

\clearpage

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/spline-concept.png}
\caption{Conceptual figure to illustrate the use of B-splines to construct nonlinear functions. In the left panel, five B-spline vectors are shown, which map values of an input variable (on the x-axis) to a value on the y-axis. The middle panel shows the same B-spline vectors, but weighted (multiplied) by real numbers, with the weights illustrated as annotations. These weighted B-spline vectors are summed to produce the values of a nonlinear function (right panel).}
\label{spline-concept} 
\end{figure}

\clearpage

\begin{figure}[htp] 
\includegraphics[width=\textwidth]{fig/ppc-counts.png}
\caption{Count predictive checks. Row one shows observed count frequencies as black points and predicted frequencies as lines. Rows two, three, and four show predicted proportions of zeros, maxima, and sums (respectively) in the training and test data, with empirical values as dashed lines. Rows two through four facilitate comparison of performance on training and test sets. Ideally, model predictions cluster around the dashed lines for both the training (x-axis direction) and test (y-axis direction) sets, leading to a tight cluster of points at the intersection of the dashed lines.}
\label{ppc-counts} 
\end{figure}

\clearpage

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/count-partial-effs.png}
\caption{Partial effects on the log-transformed negative binomial mean component of the zero-inflated negative binomial model for each level 3 ecoregion, colored by level 1 ecoregion. Lines are posterior medians. Results are similar for the zero-inflation component.}
\label{count-partial-effs}
\end{figure}

\clearpage

\begin{figure}[ht] 
\includegraphics[width=.9\textwidth]{fig/all-coefs.png}
\caption{Caterpillar plots of zero inflated negative binomial model coefficients, $\beta^{(\mu)}$ (left) and $\beta^{(\pi)}$ (right). Horizontal line segments denote 95\% credible intervals. Grey segments indicate coefficients with a less than 87\% posterior probability of being positive or negative, and colored segments indicate coefficients that are probably positive (red) or negative (blue). B-spline vectors are indicated by colons, e.g., \code{Humidity:1} indicates the first basis vector corresponding to humidity. Interactions between variables \code{a} and \code{b} are represented as \code{a x b}. Level 1 ecoregions are represented by \code{L1 ecoregion name}, and \code{L2} and \code{L3} indicate level 2 and 3 ecoregions.}
\label{all-coefs} 
\end{figure}

\clearpage

\begin{figure}[htp] 
\includegraphics[width=\textwidth]{fig/ppc-density-funs.png}
\caption{Predictive checks for burned area models. The top row shows predicted density in color and empirical density for the training set in black, which reveals overall lack of fit for the gamma and Weibull models. Row two shows the complementary cumulative distribution function (CCDF) at the tails, with 95\% and 50\% prediction intervals shown in color and observed data as black points, which shows that the Generalized Pareto distribution predicts values that are too extreme. The third and fourth rows show checks for maximum and total burned areas in the training and test set, with observed values as dashed lines and posterior draws as colored points. These final two rows facilitate checks for summary statistics on both the training and test set, with the ideal model generating predictions (colored points) clustered close to where the dashed lines intersect.}
\label{ppc-density-funs}
\end{figure}

\clearpage

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/burn-area-effs.png}
\caption{\textbf{A}. Estimated posterior medians and 95\% credible intervals for each of the 3,473 coefficients associated with expected burned area. Only one coefficient - the first basis vector for humidity - had a 95\% credible interval that excluded zero, shown in red. This effect is visualized in \textbf{B}. Partial effects of mean daily minimum humidity for each level 3 ecoregion, with posterior medians drawn as lines, and the 95\% credible intervals as ribbons. \textbf{C}. Monthly time series of expected fire sizes for every level 3 ecoregion, faceted and colored by level 1 ecoregions sorted by mean humidity. Lines are posterior medians and ribbons are 95\% credible intervals.}
\label{burn-area-effs} 
\end{figure}

\clearpage

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/max-preds-l3.png}
\caption{Posterior 99\% (light red) and 95\% (dark red) prediction intervals for the burned area of the largest fire event by month and level 3 ecoregion in the test set, shown for ecoregions with wildfires in more than 20 months. Empirical maxima are shown as black dots.}
\label{max-preds-l3}
\end{figure}

\clearpage

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig/million-acre-probs.png}
\caption{Estimated monthly posterior probabilities that one or more fire events exceed one million acres (404,686 hectares). The line represents the posterior median, and shaded region represents an 80\% credible interval. The training period up to 2010 is shown in black, and the test period for which data were withheld during parameter estimation is shown in red.}
\label{million-acre-probs}
\end{figure}

\clearpage

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/attribution-plot.png}
\caption{Posterior median contribution of each input variable to the linear predictor function of model components for the Arizona/New Mexico Mountains level 3 ecoregion from 2010-2016. A dotted vertical line marks May 2011, when the Wallow Fire ignited. Vertical positions of colored lines show contributions to the linear predictor function of each model component.}
\label{attribution-plot}
\end{figure}

\clearpage

\begin{figure}[ht] 
\includegraphics[width=\textwidth]{fig/wallow-local-conditions.png}
\caption{Daily local meteorological conditions and monthly regional mean conditions for the Wallow Fire. The blue line represents monthly averages computed over the entire Arizona/New Mexico Mountains ecoregion, and red points represent values extracted for "local" 4 km grid cells contained within the final burned area perimeter of the Wallow Fire. Vertical dashed lines indicate the dates of ignition and containment for the Wallow Fire. Units for the y-axis are given in the facet labels.}
\label{wallow-conditions}
\end{figure}

\clearpage

# Appendices {-}

\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}

## Prior specifications {-}

Prior distributions were chosen to regularize coefficients on the distribution specific means $\beta^{(\mu)}$ and structural zero parameters $\beta^{(\pi)}$. 
We used a regularized horseshoe prior on these coefficients, which shrinks irrelevant coefficients towards zero, while regularizing nonzero coefficients [@piironen2017sparsity]. 
For zero-inflated models, we used a multivariate version of the regularized horseshoe [@peltola2014hierarchical]:

$$\begin{pmatrix}
           \beta^{(\mu)}_j \\
           \beta^{(\pi)}_j
         \end{pmatrix} 
\sim 
\textnormal{N}
\Bigg(
\bm 0, 
\begin{pmatrix} 
  \tau^2_1 \tilde{\lambda}^2_{1, j} & 
    \rho \tau_1 \tau_2 \tilde{\lambda}_{1, j} \tilde{\lambda}_{2, j} \\
  \rho \tau_1 \tau_2 \tilde{\lambda}_{1, j} \tilde{\lambda}_{2, j} & 
    \tau^2_2 \tilde{\lambda}^2_{2, j}
\end{pmatrix}
\Bigg),$$

$$\tilde{\lambda}^2_{m, j} = \dfrac{c_m^2 \lambda_{j}^2}{c_m^2 + \tau_m^2 \lambda_{j}^2},$$

for each response dimension $m= 1, 2$ and coefficient $j = 1, ..., p$.
Here $\rho$ is a correlation parameter, $\tau_1$ and $\tau_2$ are global variance hyperparameters, $c_1$ and $c_2$ are hyperparameters that determine the amount of shrinkage on the largest coefficients, and $\lambda_{j}$ is a local scale parameter drawn from a half-Cauchy distribution that control the amount of shrinkage applied to coefficient $j$ [@piironen2017sparsity].
With this prior specification, information can be shared across the two response dimensions through the correlation parameter $\rho$, and/or through the local scale parameters $\lambda_j$. 
For count models without structural zeros (the Poisson and negative binomial models), this multivariate prior simplifies to a univariate regularized horseshoe prior. 

Spatiotemporal random effects were constructed using a temporally autoregressive, spatially intrinsically autoregressive formulation [@besag1995conditional; @banerjee2014hierarchical]. 
Temporarily suppressing the superscript that indicates whether the effects are on $\mu$ or $\pi$, and denoting column $t$ from an $S \times T$ $\matr{\Phi}$ as $\bm{\phi}_t$ we have:

$$\bm{\phi}_{t=1} \sim \textnormal{N}(\bm{0}, (\tau^{(\phi)}(\matr{D} - \matr{W}))^{-1})$$

$$\bm{\phi}_{t} \sim \textnormal{N}(\eta \bm{\phi}_{t - 1}, (\tau^{(\phi)}(\matr{D} - \matr{W}))^{-1}), \quad t = 2, ..., T$$

where $\eta$ is a temporal dependence parameter, $\tau^{(\phi)}$ is a precision parameter, $\matr D$ is an $S \times S$ diagonal matrix with entries corresponding to the number of spatial neighbors for each spatial unit, and $\matr W$ is an $S \times S$ spatial adjacency matrix with nonzero elements only when spatial unit $i$ is a neighbor of spatial unit $j$ ($w_{i, j} = 1$ if $i$ is a neighbor of $j$, and $w_{i, j} = 0$ otherwise, including $w_{i, i} = 0$ for all $i$). 
$\tau^{(\phi)}$ is a precision parameter. 
We imposed a soft identifiability constraint that places high prior mass near $\sum_{s = 1}^S \phi^*_{t, s} = 0$ for all $t$.

We applied a univariate regularized horseshoe prior to all $\beta$ coefficients in burned area models [@piironen2017sparsity]:

$$ \beta_j
\sim 
\textnormal{N}
\big(
0, 
\tau^2 \tilde{\lambda}^2_{j}
\big), 
\quad
\tilde{\lambda}^2_{j} = \dfrac{c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2},$$

Spatiotemporal random effects were constructed in the same way as for the count models.

\newpage

## Joint distributions {-}

Here we provide the unnormalized posterior densities for each model.
Square brackets represent a probability mass or density function. 
Parameterizations for model likelihoods are provided first, followed by the factorization of the joint distribution, with explicit priors.


### Poisson wildfire count model {-}

We used the following parameterization of the Poisson distribution: 

$$[n | \mu] = \dfrac{\mu^ne^{-\mu}}{n!},$$ 
where $\mu$ is the mean and variance.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\phi}, \sigma^{(\phi)}, \eta, \bm{\lambda}, c, \tau \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \phi_{s, t}] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)} | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\sigma^{(\phi)}] [\eta] [c] [\tau] [\alpha^{(\mu)}] \\
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{Poisson}(n_{s, t} | \textnormal{exp}(\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi_{s, t})) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j^{(\mu)} | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \\ 
&& \textnormal{Normal}^+(\tau | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2).
\end{flalign*}

\newpage

### Negative binomial wildfire count model {-}

We used the following parameterization of the negative binomial distribution: 
$$[n | \mu, \delta] = \binom{n + \delta - 1}{n} \big( \frac{\mu}{\mu + \delta} \big)^n \big( \frac{\delta}{\mu + \delta} \big)^\delta,$$ 

where $\mu$ is the mean, and $\delta$ is a dispersion parameter.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\phi}, \sigma^{(\phi)}, \eta, \bm{\lambda}, c, \tau, \delta \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \phi_{s, t}, \delta] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)} | \lambda_j, c, \tau] [\lambda_j] \times \\ 
&& [\sigma^{(\phi)}] [\eta] [c][\tau] [\alpha^{(\mu)}] [\delta] \\
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{Negative Binomial}(n_{s, t} | \textnormal{exp}(\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi_{s, t}), \delta) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j^{(\mu)} | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \\ 
&& \textnormal{Normal}^+(\tau | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2) \times \textnormal{Normal}^+(\delta | 0, 5^2).
\end{flalign*}

\newpage

### Zero-inflated Poisson wildfire count model {-}

We used the following parameterization of the zero-inflated Poisson distribution: 

$$[n | \mu, \pi] = I_{n=0} (1-\pi + \pi e ^{-\mu}) + I_{n > 0} \pi \frac{\mu^ne^{-\mu}}{n!},$$
where $\mu$ is the Poisson mean, and $1-\pi$ is the probability of an extra zero.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, 
 \bm{\phi}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}, 
 \bm{\phi}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}, 
 \bm{\lambda}, c, \tau, \rho \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, \phi_{s, t}^{(\mu)}, \phi_{s, t}^{(\pi)}] \times \\
&& [\bm{\phi}_1^{(\mu)} | \sigma^{(\phi, \mu)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\mu)} | \bm{\phi}_{t - 1}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}] \times \\
&& [\bm{\phi}_1^{(\pi)} | \sigma^{(\phi, \pi)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\pi)} | \bm{\phi}_{t - 1}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)}, \beta_j^{(\pi)} | \lambda_j, c, \tau, \rho] [\lambda_j] \times \\
&& [\sigma^{(\phi, \mu)}] [\sigma^{(\phi, \pi)}] [\eta^{(\mu)}] [\eta^{(\pi)}]
[\alpha^{(\mu)}] [\alpha^{(\pi)}] [\rho] \prod_{m = 1}^2 [c_m] [\tau_m] \\
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{ZIP}(n_{s, t} | e^{\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi^{(\mu)}_{s, t}}, \textnormal{logit}^{-1}(\alpha^{(\pi)} + \matr{X}_{(s, t)} \bm{\beta}^{(\pi)} + \phi^{(\pi)}_{s, t})) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\mu)}_1 | \bm{0}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\mu)}_{t} | \eta^{(\mu)} \bm{\phi}^{(\mu)}_{t - 1}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\pi)}_1 | \bm{0}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\pi)}_{t} | \eta^{(\pi)} \bm{\phi}^{(\pi)}_{t - 1}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{N}
\Bigg(\begin{pmatrix}
           \beta^{(\mu)}_j \\
           \beta^{(\pi)}_j
         \end{pmatrix} \Big{|} \,
\bm 0, 
\begin{pmatrix} 
  \tau^2_1 \frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2} & 
    \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} \\
  \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} & 
    \tau^2_2 \frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}
\end{pmatrix}
\Bigg) \times \\
&& \prod_{j = 1}^p \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&&\textnormal{Normal}^+(\sigma^{(\phi, \mu)} | 0, 1^2) \times \textnormal{Normal}^+(\sigma^{(\phi, \pi)} | 0, 1^2) \times \\
&& \textnormal{Beta}(\eta^{(\mu)} | 1, 1) \times \textnormal{Beta}(\eta^{(\pi)} | 1, 1) \times \\
&& \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\pi)} | 0, 5^2) \times \textnormal{LKJ}(\rho | 3) \times \\
&& \prod_{m = 1}^2 \textnormal{Inv-Gamma}(c_m^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau_m | 0, 5^2).
\end{flalign*}

\newpage 

### Zero-inflated negative binomial wildfire count model {-}

We used the following parameterization of the zero-inflated negative binomial distribution: 
$$[n|\mu, \delta, \pi] = I_{n=0} (1-\pi + \pi \big( \frac{\delta}{\mu + \delta} \big)^\delta) + I_{n > 0} \binom{n + \delta - 1}{n} \big( \frac{\mu}{\mu + \delta} \big)^n \big( \frac{\delta}{\mu + \delta} \big)^\delta,$$ 
where $\mu$ is the negative binomial mean, $\delta$ is the negative binomial dispersion, and , and $1-\pi$ is the probability of an extra zero.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, 
 \bm{\phi}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}, 
 \bm{\phi}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}, 
 \bm{\lambda}, c, \tau, \rho, \delta \mid \matr{N}] \propto \\
&& \prod_{s = 1}^S \prod_{t = 1}^T [n_{s, t} | \bm{\beta}^{(\mu)}, \alpha^{(\mu)}, \bm{\beta}^{(\pi)}, \alpha^{(\pi)}, \phi_{s, t}^{(\mu)}, \phi_{s, t}^{(\pi)}, \delta] \times \\
&& [\bm{\phi}_1^{(\mu)} | \sigma^{(\phi, \mu)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\mu)} | \bm{\phi}_{t - 1}^{(\mu)}, \sigma^{(\phi, \mu)}, \eta^{(\mu)}] \times \\
&& [\bm{\phi}_1^{(\pi)} | \sigma^{(\phi, \pi)}] \prod_{t = 2}^T [\bm{\phi}_t^{(\pi)} | \bm{\phi}_{t - 1}^{(\pi)}, \sigma^{(\phi, \pi)}, \eta^{(\pi)}] \times \\
&& \prod_{j = 1}^p [\beta_j^{(\mu)}, \beta_j^{(\pi)} | \lambda_j, c, \tau, \rho] [\lambda_j] \times \\
&& [\sigma^{(\phi, \mu)}] [\sigma^{(\phi, \pi)}] [\eta^{(\mu)}] [\eta^{(\pi)}]
[\alpha^{(\mu)}] [\alpha^{(\pi)}] [\rho] [\delta] \prod_{m = 1}^2 [c_m] [\tau_m].
\end{flalign*}

\begin{flalign*}
&& = \prod_{s = 1}^S \prod_{t = 1}^T \textnormal{ZINB}(n_{s, t} | e^{\alpha^{(\mu)} + \matr{X}_{(s, t)} \bm{\beta}^{(\mu)} + \phi^{(\mu)}_{s, t}}, \delta, \textnormal{logit}^{-1}(\alpha^{(\pi)} + \matr{X}_{(s, t)} \bm{\beta}^{(\pi)} + \phi^{(\pi)}_{s, t})) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\mu)}_1 | \bm{0}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\mu)}_{t} | \eta^{(\mu)} \bm{\phi}^{(\mu)}_{t - 1}, ((\sigma^{(\phi, \mu)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \textnormal{Normal}(\bm{\phi}^{(\pi)}_1 | \bm{0}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}^{(\pi)}_{t} | \eta^{(\pi)} \bm{\phi}^{(\pi)}_{t - 1}, ((\sigma^{(\phi, \pi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{N}
\Bigg(\begin{pmatrix}
           \beta^{(\mu)}_j \\
           \beta^{(\pi)}_j
         \end{pmatrix} \Big{|} \,
\bm 0, 
\begin{pmatrix} 
  \tau^2_1 \frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2} & 
    \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} \\
  \rho \tau_1 \tau_2 \sqrt{\frac{c_1^2 \lambda_{j}^2}{c_1^2 + \tau_1^2 \lambda_{j}^2}} \sqrt{\frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}} & 
    \tau^2_2 \frac{c_2^2 \lambda_{j}^2}{c_2^2 + \tau_2^2 \lambda_{j}^2}
\end{pmatrix}
\Bigg) \times \\
&& \prod_{j = 1}^p \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&&\textnormal{Normal}^+(\sigma^{(\phi, \mu)} | 0, 1^2) \times \textnormal{Normal}^+(\sigma^{(\phi, \pi)} | 0, 1^2) \times \\
&& \textnormal{Beta}(\eta^{(\mu)} | 1, 1) \times \textnormal{Beta}(\eta^{(\pi)} | 1, 1) \times \\
&& \textnormal{Normal}(\alpha^{(\mu)} | 0, 5^2) \times \textnormal{Normal}(\alpha^{(\pi)} | 0, 5^2) \times \textnormal{LKJ}(\rho | 3)  \times \textnormal{Normal}^+(\delta | 0, 5^2) \times \\
&& \prod_{m = 1}^2 \textnormal{Inv-Gamma}(c_m^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau_m | 0, 5^2).
\end{flalign*}

\newpage 

### Generalized Pareto/Lomax burned area model {-}

We used the following parameterization of the GPD/Lomax distribution: 
$$[y|\sigma, \kappa] = \dfrac{1}{\sigma} \Big( \dfrac{\kappa y}{\sigma} + 1 \Big) ^ {- (\kappa + 1)\kappa^{-1}},$$ 
where $\kappa$ is a shape parameter and $\sigma$ is a scale parameter. 

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \kappa^{(L)}, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \bm{\beta}, \alpha, \phi_{s_i, t_i}, \kappa^{(L)}]  \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\ 
&& [\alpha] [c] [\tau] [\kappa^{(L)}] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&&= \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Lomax}(y_i | \kappa^{(L)}, e^{\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}}) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \\
&& \textnormal{Normal}^+(\kappa^{(L)} | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

### Tapered Pareto burned area model {-}

We used the following parameterization of the tapered Pareto distribution: 
$$[y|\kappa, \nu] = \Big( \dfrac{\kappa}{y} + \dfrac{1}{\nu} \Big) \textnormal{exp} (-x / \nu),$$ 
where $\kappa$ is a shape parameter and $\nu$ a taper parameter.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \nu, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \bm{\beta}, \alpha, \phi_{s_i, t_i}, \nu] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\nu] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&&= \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Tapered Pareto}(y_i | e^{\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}}, \nu) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Cauchy}^+(\nu | 0, 1) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage


### Lognormal burned area model {-}

We used the following parameterization of the lognormal distribution: 
$$[y | \mu, \sigma] = \dfrac{1}{y} \dfrac{1}{\sigma \sqrt[]{2 \pi}} \textnormal{exp}\Big( -\dfrac{(\log (y) - \mu)^2}{2 \sigma^2} \Big),$$
where $\mu$ and $\sigma$ are location and scale parameters, respectively.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \sigma, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \beta, \alpha, \phi_{s_i, t_i}, \sigma] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\sigma] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&&= \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Lognormal}(y_i | \alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i}, \sigma) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Normal}^+(\sigma | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

### Gamma burned area model {-}

We used the following parameterization of the gamma distribution: 
$$[y | \kappa, \sigma] = \dfrac{1}{\Gamma (\kappa) \sigma^\kappa} y^{\kappa - 1} \textnormal{exp}(-y / \sigma),$$ 
where $\kappa$ is a shape parameter and $\sigma$ a scale parameter.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \kappa, \bm{\lambda}, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \beta, \alpha, \phi_{s_i, t_i}, \kappa] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\kappa] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&& = \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Gamma}(y_i | \kappa, \kappa / \textnormal{exp}(\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i})) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Normal}^+(\kappa | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

### Weibull burned area model {-}

We used the following parameterization of the Weibull distribution: 
$$[y | \kappa, \sigma] = \dfrac{\kappa}{\sigma} \Big( \dfrac{y}{\sigma} \Big)^{\kappa - 1} \textnormal{exp} \Big( - \Big(\dfrac{y}{\sigma} \Big)^\alpha \Big),$$ 
where $\kappa$ is a shape parameter and $\sigma$ is a scale parameter.

The unnormalized posterior density of this model is:

\begin{flalign*}
&& [\bm{\beta}, \alpha, \bm{\phi}, \sigma^{(\phi)}, \eta, \kappa, \lambda, c, \tau \mid \bm{y}] \propto \\
&& \prod_{i = 1}^{n_{\textnormal{tot}}} [y_i | \beta, \alpha, \phi_{s_i, t_i}, \kappa] \times \\
&& [\bm{\phi}_1 | \sigma^{(\phi)}] \prod_{t = 2}^T [\bm{\phi}_t | \bm{\phi}_{t - 1}, \sigma^{(\phi)}, \eta] \times \\
&& \prod_{j = 1}^p [\beta_j | \lambda_j, c, \tau] [\lambda_j] \times \\
&& [\alpha] [c] [\tau] [\kappa] [\eta] [\sigma^{(\phi)}]
\end{flalign*}

\begin{flalign*}
&& = \prod_{i = 1}^{n_{\textnormal{tot}}} \textnormal{Weibull}(y_i | \kappa, \textnormal{exp}(\alpha + \matr{X}_{(s_i, t_i)} \bm{\beta} + \phi_{s_i, t_i})) \times \\
&& \textnormal{Normal}(\bm{\phi}_1 | \bm{0}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{t = 2}^T \textnormal{Normal}(\bm{\phi}_{t} | \eta \bm{\phi}_{t - 1}, ((\sigma^{(\phi)})^{-2} (\matr{D} - \matr{W}))^{-1}) \times \\
&& \prod_{j = 1}^p \textnormal{Normal}\bigg(\beta_j | 0, \frac{\tau^2 c^2 \lambda_{j}^2}{c^2 + \tau^2 \lambda_{j}^2}\bigg) \times \textnormal{Cauchy}^+(\lambda_{j} | 0, 1) \times \\
&& \textnormal{Normal}(\alpha | 0, 5^2) \times \textnormal{Inv-Gamma}(c^2 | 2.5, 10) \times \textnormal{Normal}^+(\tau | 0, 5^2) \times \\
&& \textnormal{Normal}^+(\kappa | 0, 5^2) \times \textnormal{Beta}(\eta | 1, 1) \times \textnormal{Normal}^+(\sigma^{(\phi)} | 0, 1^2).
\end{flalign*}

\newpage

